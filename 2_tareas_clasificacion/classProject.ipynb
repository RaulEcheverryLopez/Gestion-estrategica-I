{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto de Aprendizaje Supervisado\n",
    "- Con este proyecto aprenderemos como construir, entrenar y evaluar un modelo que resuelva una tarea de clasificación.\n",
    "- Utilizaremos diversas librerías para la manipulación, análisis, visualización, modelado y evaluación de los datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Librerías a básicas utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "\n",
    "# Manejo de analísis de datos a través de dataframes (data tabular)\n",
    "import pandas as pd\n",
    "# Manipulación de arreglos y análisis numérico\n",
    "import numpy as np\n",
    "# Visualización de datos\n",
    "import seaborn as sns\n",
    "# Visualización de datos\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Diccionario de datos\n",
    "\n",
    "1. symboling: -3, -2, -1, 0, 1, 2, 3.\n",
    "2. normalized-losses: continuous from 65 to 256. \n",
    "3. make: alfa_romero, audi, bmw, chevrolet, dodge, honda, isuzu, jaguar, mazda, mercedes-benz, mercury, mitsubishi, nissan, peugot, plymouth, porsche, renault, saab, subaru, toyota, volkswagen, volvo\n",
    "4. fuel_type: diesel, gas.\n",
    "5. aspiration: std, turbo.\n",
    "6. num_of_doors: four, two.\n",
    "7. body_style: hardtop, wagon, sedan, hatchback, convertible. \n",
    "8. drive_wheels: 4wd, fwd, rwd.\n",
    "9. engine_location: front, rear.\n",
    "10. wheel_base: continuous from 86.6 120.9.\n",
    "11. length: continuous from 141.1 to 208.1.\n",
    "12. width: continuous from 60.3 to 72.3.\n",
    "13. height: continuous from 47.8 to 59.8.\n",
    "14. curb_weight: continuous from 1488 to 4066.\n",
    "15. engine_type: dohc, dohcv, l, ohc, ohcf, ohcv, rotor.\n",
    "16. num_of_cylinders: eight, five, four, six, three, twelve, two. \n",
    "17. engine_size: continuous from 61 to 326.\n",
    "18. fuel_system: 1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.\n",
    "19. bore: continuous from 2.54 to 3.94.\n",
    "20. stroke: continuous from 2.07 to 4.17.\n",
    "21. compression_ratio: continuous from 7 to 23.\n",
    "22. horse_power: continuous from 48 to 288.\n",
    "23. peak_rpm: continuous from 4150 to 6600. \n",
    "24. city_mpg: continuous from 13 to 49.\n",
    "25. highway_mpg: continuous from 16 to 54. \n",
    "26. price: continuous from 5118 to 45400.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Lectura de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturamos los datos a partir de un archivo csv. Al no tener los nombres de las columnas pero sí un diccionario de datos, renombramos al momento de obtener el dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('imports-85.csv',names=['symboling','normalized_losses','make','fuel_type','aspiration',\n",
    "                                            'num_of_doors','body_style','drive_wheels','engine_location',\n",
    "                                            'wheel_base','length','width','height','curb_weight',\n",
    "                                            'engine_type','num_of_cylinders','engine_size','fuel_system',\n",
    "                                            'bore','stroke','compression_ratio','horse_power','peak_rpm',\n",
    "                                            'city_mpg','highway_mpg','price'],index_col=False)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Análisis Exploratorio de Datos\n",
    "\n",
    "- Limpieza de datos\n",
    "- Exploración de datos\n",
    "- Ingeniería de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores faltantes y tipos de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a revisar los datos faltantes (explícitos e implícitos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al revisar los datos básicos observamos que a primera vista no existen datos faltantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero, tan sólo al analizar los primeros registros del dataset (e.g. normalized_losses), nos damos cuenta de datos con valores = ?. Esto, claramente, puede indicar datos faltantes. Para ratificar esta sospecha, tenemos que mirar en que columnas pueden estar apareciendo estos valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in df.select_dtypes(include=['object']).columns:\n",
    "    cantidad = df[df[var] == '?'][var].count()\n",
    "    print(f\"Existen {cantidad} valores en {var} con el valor ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Contamos, de todas las variables de tipo objeto (pues son las únicas que pueden contener un valor ?), contamos los valores en los que un valor ? aparece.\n",
    "\n",
    "- A partir de los resutados podemos observar que existen 7 variables con valores de este tipo.\n",
    "\n",
    "- Al tener tan pocos registros, no es bueno eliminar, así sean tan pocos, pero si queremos imputarlos debemos cambiarlos a nan\n",
    "\n",
    "- De igual manera no podemos cambiar el valor de una variable categórica a numérica si existen datos con ?\n",
    "\n",
    "- Cambiaremos los valores ? por nan, pero primero revisaremos tipos de datos inadecuados para todas las variables del dataset.\n",
    "\n",
    "- Revisando detalladamente encontramos incongruencias ya que algunas variables tienen datos de distinto tipo al manifestado en el diccionario de datos\n",
    "    - symboling debería ser categórico\n",
    "    - normalized-losses debería ser númerico y no categórico\n",
    "    - bore (diámetro del motor) debería ser numérico\n",
    "    - stroke debe ser numérico\n",
    "    - horse-power debe ser numérico\n",
    "    - peak-rpm debe ser numérico\n",
    "    - price debe ser numérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de meternos la limpieza de datos debemos ver como están distribuidas las categorías de la clase objetivo symboling. Es importante conocer si nuestro conjunto de datos se encuentra balanceado o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.countplot(x=\"symboling\", data=df, palette = \"Set1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construímos una tabla para ver las cantidades para cada categoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=df[\"symboling\"], columns=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pocos valores para riesgo -2. Tiene sentido agrupar aquellos carros con riesgo negativo.\n",
    "\n",
    "Teniendo en cuenta lo observado sobre la variable objetivo, creemas un transformer personalizado para realizar los cambios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función para cambiar los tipos de las variables independientes. Igualmente hacemos uso de un diccionario para transformar nuestros valores categóricos ordinales a numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Transformer personalizado formatea la variable objetivo\n",
    "# Agrupamos valores con riesgo negativo\n",
    "# Convertimos el tipo de la variable objetivo a categórica\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class Format_target(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        result = X.copy()\n",
    "        result.loc[(result.symboling < 0), \"symboling\"] = -1\n",
    "        result['symboling'] = result['symboling'].astype(int).astype(str)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modified = Format_target().fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.countplot(x='symboling',data=df_modified, palette='Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos la tabla luego del cambio\n",
    "\n",
    "pd.crosstab(index=df_modified[\"symboling\"], columns=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro conjunto de datos parece más balanceado con respecto a la variable objetivo. Incluso, antes de realizar más cambios podemos observar la línea base que servirá como punto de partida  para los modelos que vamos a implementar. Al estar trabajando con una tarea de clasificación, utilizamos la clase mayoritaria, en este caso es el riesgo neutro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modified['symboling'][df_modified['symboling'] == '0'].count()/df.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora trabajaremos con los problemas encontrados en las variables independientes con respecto a valores que podrían ser faltantes y tipos de datos distintos a los indicados en el diccionario de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(df,columns,val):\n",
    "    result = df.copy()\n",
    "    for col in columns:\n",
    "        result[col] = result[col].astype(val)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Transformer personalizado que nos cambia los valores ? por nulos\n",
    "# Igualmente nos modifica los tipos de variables errados\n",
    "class Format_variables(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        result = X.copy()\n",
    "        result = result.replace('?',np.nan)\n",
    "        result = convert(result,['normalized_losses','compression_ratio','peak_rpm',\n",
    "            'bore','stroke','price','horse_power'],'float64')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modified = Format_variables().fit_transform(df_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos que los tipos de datos son los correctos\n",
    "# Asimismo nos damos cuenta que ahora sí tenemos datos faltantes\n",
    "df_modified.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dibujamos un mapa de calor para vislumbrar la ubicación de los valores faltantes\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df_modified.isnull(), yticklabels = False, cbar = False, cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modified.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada columna tiene 205 registros y de las 26 (25 independientes y 1 dependiente) tenemos 7 con datos faltantes.\n",
    "\n",
    "- normalized_losses: 41 faltantes\n",
    "- num_of_doors: 2 faltantes\n",
    "- bore: 4 faltantes\n",
    "- stroke: 4 faltantes\n",
    "- horsepower: 2 faltantes\n",
    "- peak-rpm: 2 faltantes\n",
    "- price: 4 faltantes\n",
    "\n",
    "¿Cómo manejar estos valores faltantes?\n",
    "- Eliminar columnas o registros.\n",
    "- Imputar con la media, frecuencia o con base en alguna función\n",
    "\n",
    "En este caso trabajaremos de la siguiente manera:\n",
    "- Imputación con la media (numéricas):\n",
    "    - normalized_losses\n",
    "    - stroke\n",
    "    - bore\n",
    "    - horse_power\n",
    "    - peak_rpm\n",
    "    - price\n",
    "- Imputación con la moda:\n",
    "    - num_of_doors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Transformer personalizado que nos imputa los valores faltantes\n",
    "# Utiliza la media para columnas numéricas y la moda para las categóricas\n",
    "class Imputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cont, cat):\n",
    "        self.cont = cont\n",
    "        self.cat = cat\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        result = X.copy()\n",
    "        for c in self.cont:\n",
    "            avg = result[c].mean(axis=0)\n",
    "            result[c].replace(np.nan,avg,inplace=True)\n",
    "        for c in self.cat:\n",
    "            mode = result[c].value_counts().idxmax()\n",
    "            result[c].replace(np.nan,mode,inplace=True)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = ['normalized_losses', 'stroke', 'bore', 'horse_power', 'peak_rpm', 'price']\n",
    "cat = ['num_of_doors']\n",
    "\n",
    "df_modified = Imputer(cont,cat).fit_transform(df_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modified.info(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dibujamos un mapa de calor para vislumbrar la ubicación de los valores faltantes\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df_modified.isnull(), yticklabels = False, cbar = False, cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que ya no contamos con valores faltantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores atípicos\n",
    "Visualizamos utilizando\n",
    "- Diagramas de cajas y bigotes\n",
    "- Histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para graficar los valores continuos del dataframe\n",
    "def outliers_visual(data):\n",
    "    cont_vars = list(data.select_dtypes('number').columns)\n",
    "    plt.figure(figsize=(15, 40))\n",
    "    i = 0\n",
    "    val = int(len(cont_vars)/2) + 1\n",
    "    for col in cont_vars:\n",
    "        i += 1\n",
    "        plt.subplot(val, 4, i)\n",
    "        plt.boxplot(data[col])\n",
    "        plt.title('{} boxplot'.format(col))\n",
    "        i += 1\n",
    "        plt.subplot(val, 4, i)\n",
    "        plt.hist(data[col])\n",
    "        plt.title('{} histogram'.format(col))\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_visual(df_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando los gráficos anteriores podemos observar las distribuciones para cada variable continua.\n",
    "\n",
    "Visualmente resulta claro que existen outliers para algunas de las variables dentro del dataset:\n",
    "- normalized_losses\n",
    "- wheel_base\n",
    "- width\n",
    "- engine_size\n",
    "- stroke\n",
    "- compression_ratio\n",
    "- horse_power\n",
    "- city_mpg\n",
    "- highway_mpg\n",
    "- price\n",
    "\n",
    "Podemos corroborar lo anterior de manera estadística haciendo uso del método de Tukey (se consideran como datos atípicos aquellos que están 1.5 veces por fuera del rango intercuartil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular los datos atípicos utilizando el método de Tukey\n",
    "def outlier_count(col, data):\n",
    "    print(15*'-' + col + 15*'-')\n",
    "    q75, q25 = np.percentile(data[col], [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    min_val = q25 - (iqr*1.5)\n",
    "    max_val = q75 + (iqr*1.5)\n",
    "    outlier_count = len(np.where((data[col] > max_val) | (data[col] < min_val))[0])\n",
    "    outlier_percent = round(outlier_count/len(data[col])*100, 2)\n",
    "    print('Number of outliers: {}'.format(outlier_count))\n",
    "    print('Percent of data that is outlier: {}%'.format(outlier_percent))\n",
    "    return outlier_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar las columnas de tipo continuas con datos atípicos\n",
    "cont_vars = []\n",
    "for col in list(df_modified.select_dtypes('number').columns):\n",
    "    if outlier_count(col, df_modified) > 0:\n",
    "        cont_vars.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo en cuenta los gráficos previos y los estadísticos, se puede ver que existen cantidades diferentes de outliers para cada variable y hacia diferentes direcciones. Limitaremos a través de winsorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "wins_dict = {}\n",
    "\n",
    "def test_wins(col, df, wins_dict, lower_limit=0, upper_limit=0, show_plot=True):\n",
    "    wins_data = winsorize(df[col], limits=(lower_limit, upper_limit))\n",
    "    wins_dict[col] = wins_data\n",
    "    if show_plot == True:\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.subplot(121)\n",
    "        plt.boxplot(df[col])\n",
    "        plt.title('original {}'.format(col))\n",
    "        plt.subplot(122)\n",
    "        plt.boxplot(wins_data)\n",
    "        plt.title('wins=({},{}) {}'.format(lower_limit, upper_limit, col))\n",
    "        plt.show()\n",
    "    return wins_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificación de la winsorizing\n",
    "wins_dict = {}\n",
    "wins_dict = test_wins(cont_vars[0], df_modified, wins_dict, upper_limit=.04, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[1], df_modified, wins_dict, upper_limit=.015, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[2], df_modified, wins_dict, lower_limit=.005, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[3], df_modified, wins_dict, upper_limit=.04, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[4], df_modified, wins_dict, upper_limit=.05, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[5], df_modified, wins_dict, lower_limit=.09, upper_limit=.04, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[6], df_modified, wins_dict, lower_limit=.09, upper_limit=.11, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[7], df_modified, wins_dict, upper_limit=.03, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[8], df_modified, wins_dict, upper_limit=.01, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[9], df_modified, wins_dict, upper_limit=.01, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[10], df_modified, wins_dict, upper_limit=.015, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[11], df_modified, wins_dict, upper_limit=.12, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Transformer personalizado para limitar los outliers con el winsorizing\n",
    "class Winsorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, wins, cont_vars):\n",
    "        self.wins = wins\n",
    "        self.cont_vars = cont_vars\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        result = X.copy()\n",
    "        for col in cont_vars:\n",
    "            result[col] = self.wins[col]     \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wins = Winsorizer(wins_dict,cont_vars).fit_transform(df_modified)\n",
    "outliers_visual(df_wins)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Exploración de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis univariado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptores estadísticos para valores continuos\n",
    "df_wins.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptores estadísticos para valores categóricos\n",
    "df_wins.describe(include='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para visualizar las distribuciones de las variables continuas\n",
    "def visualize_distributions(df):\n",
    "    cols = df.select_dtypes('number').columns\n",
    "    val = int(len(cols)/2)+1\n",
    "    plt.figure(figsize=(15, 40))\n",
    "    for i, col in enumerate(cols, 1):\n",
    "        plt.subplot(val, 4, i)\n",
    "        plt.hist(df[col])\n",
    "        plt.title(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos la distribución de las variables continuas\n",
    "visualize_distributions(df_wins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de realizar la limpieza de datos, imputando datos y winsorizing, logramos percibir distribuciones menos segadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wins.make.value_counts().nlargest(10).plot(kind='bar', figsize=(15,5))\n",
    "plt.title(\"Number of vehicles by make\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Make')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wins['aspiration'].value_counts().plot.pie(figsize=(6, 6), autopct='%.2f')\n",
    "plt.title(\"Fuel type pie diagram\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Fuel type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wins.horse_power[np.abs(df_wins.horse_power-df_wins.horse_power.mean())<=(3*df_wins.horse_power.std())].hist(bins=5,color='blue');\n",
    "plt.title(\"Horse power histogram\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Horse power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wins['curb_weight'].hist(bins=5,color='blue')\n",
    "plt.title(\"Curb weight histogram\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Curb weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wins['drive_wheels'].value_counts().plot(kind='bar',color='blue')\n",
    "plt.title(\"Drive wheels diagram\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Drive wheels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wins['num_of_doors'].value_counts().plot(kind='bar',color='blue')\n",
    "plt.title(\"Number of doors frequency diagram\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Number of doors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wins['normalized_losses'].hist(bins=5,color='blue')\n",
    "plt.title(\"Normalized losses of vehicles\")\n",
    "plt.ylabel('Number of vehicles')\n",
    "plt.xlabel('Normalized losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir del anterior del anterior análisis obtenemos los siguientes hallazgos:\n",
    "\n",
    "- Toyate es la comàñía que tiene más vehículos, más del 40% más que Nissan \n",
    "- El tipo de combustible más popular es el estándar con más del 80%.\n",
    "- Front wheel drive es la primera elección en vehículos, siguidos de rear wheel y four wheel (muy pocos de estos últimos). \n",
    "- Curb weight está distribuida entre 1500 y 4000 aproximadamente\n",
    "- Normalized losses, que no es más que el promedio de pérdidas por vehículo asegurado al año, tiene más número de carros en el rango de 90 a 140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis Bivariado\n",
    "\n",
    "Haciendo uso del diagrama de cajas y bigotes obtendremos un resumen visual de la distribución de cada una de las variables independientes con respecto a la dependiente, observando así la distribución de los distintos grupos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,50))\n",
    "i=1\n",
    "\n",
    "for var_num in df_wins.select_dtypes(include=['int64','float64']):\n",
    "    ax = fig.add_subplot(8, 2, i)\n",
    "    sns.boxplot(x=df_wins.columns[0], y=var_num, data=df_wins, palette='Set1')\n",
    "    plt.title(var_num)\n",
    "    i+=1\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conociendo un poco acerca de las distribuciones de las variables con respecto a la variable objetivo, nos podemos dar una primera idea sobre cuáles características podrían resultar siendo más importantes al momento de influir en la variable objetivo. Esto resulta útil pues al incluirlas, nuestro modelo puede brindarnos mejores resultados en cuanto a correctitud de la predicción.\n",
    "\n",
    "Observando las medianas de cada grupo (características) nos puede mostrar si verdaderamente existe una relación entre dicha variable independiente y la objetivo.\n",
    "\n",
    "El tamaño de la caja nos puede mostrar la variabilidad dentro de esa característica.\n",
    "\n",
    "Por ejemplo podemos notar que:\n",
    "\n",
    "- normalized_losses evidentemente indica una relación directamente proporcional con el riesgo. Es decir, mayores pérdidas claramente indica mayor riesgo.\n",
    "- wheel_base más alto, al igual que largo y ancho del vehículo parece asociarse con riesgo negativo o neutro\n",
    "- mayor tamaño del motor asociado a mayor riesgo\n",
    "\n",
    "Para ratificar algunos de estos descubrimientos utilizamos un diagrama de violín. Al ser estos unos diagramas que combinan los boxplots y los diagramas de densidad, nos pueden dar una noción de qué variables independientes pueden sernos de mayor untilidad para distinguir entre las distintas categorías de la variable objetivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,50))\n",
    "i=1\n",
    "\n",
    "for var_num in df_wins.select_dtypes(include=['int64','float64']):\n",
    "    ax = fig.add_subplot(8, 2, i)\n",
    "    sns.violinplot(x=df_wins.columns[0], y=var_num, data=df_wins, palette='Set1')\n",
    "    plt.title(var_num)\n",
    "    i+=1\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ubicar los registros de acuerdo a su categoría, de manera que podamos dibujar nuestro diagrama de densidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categories(df):\n",
    "    risks = {}\n",
    "    risks['neg'] = df.loc[df.symboling == \"-1\"]\n",
    "    risks['neutral'] = df.loc[df.symboling == \"0\"]\n",
    "    risks['1'] = df.loc[df.symboling == \"1\"]\n",
    "    risks['2'] = df.loc[df.symboling == \"2\"]\n",
    "    risks['3'] = df.loc[df.symboling == \"3\"]\n",
    "    return risks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el diagrama de densidad se puede observar aquellas variables independientes que mejor \n",
    "discriminan las clases de la variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risks = categories(df_wins)\n",
    "\n",
    "i=1\n",
    "fig = plt.figure(figsize=[20, 50])\n",
    "for var_num in df_wins.select_dtypes('number').columns:\n",
    "    ax = fig.add_subplot(8, 2, i)\n",
    "    sns.kdeplot(risks['neg'][[var_num]].iloc[:,0], shade=True, color=\"b\", label=\"Negative risk\",palette='Set1')\n",
    "    sns.kdeplot(risks['neutral'][[var_num]].iloc[:,0], shade=True, color=\"r\", label=\"Neutral risk\",palette='Set1')\n",
    "    sns.kdeplot(risks['1'][[var_num]].iloc[:,0], shade=True, color=\"g\", label=\"Risk = 1\",palette='Set1')\n",
    "    sns.kdeplot(risks['2'][[var_num]].iloc[:,0], shade=True, color=\"m\", label=\"Risk = 2\",palette='Set1')\n",
    "    sns.kdeplot(risks['3'][[var_num]].iloc[:,0], shade=True, color=\"c\", label=\"Risk = 3\",palette='Set1')\n",
    "    plt.legend()\n",
    "    i+=1\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando los gráficos de densidad logramos percibir que algunas features nos permiten distinguir la variable objetivo, mientras otras no nos brindan mayor información.\n",
    "\n",
    "Dentro de las candidatas podríamos tener:\n",
    "- normalized_losses\n",
    "- wheel_base\n",
    "- length\n",
    "- city_mpg\n",
    "- highway_mpg\n",
    "- price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = sns.pairplot(df_wins, vars=['normalized_losses','wheel_base', 'length', \n",
    "                                 'city_mpg', 'highway_mpg', 'price'], hue=\"symboling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haciendo uso de los pairplots de aquellas variables que mejor discriminaban la variable objetivo, intentamos encontrar combinaciones que nos ayuden a distinguir mejor los 5 grupos de riesgo que tenemos en nuestra variable objetivo symboling.\n",
    "\n",
    "Definitivamente no resulta claro un par de variables que nos discrimene la variable objetivo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de correlación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar la matriz de correlación para corroborar los indicios sobre las relaciones presentes entre algunas de las características y la variable objetivo.\n",
    "\n",
    "El uso de la matriz de correlación es importante para conocer la correlación entre cada par de variables\n",
    "- Las variables independientes a utilizar deberían estar altamente correlacionadas con la objetivo\n",
    "- Se deben evitar problemas de multicolinealidad\n",
    "- Las variables independientes no deben estar correlacionadas entre sí\n",
    "- Si las variables independientes se encuentran altamente correlacionadas, el modelo puede presentar problemas al momento de entrenarlo e interpretarlo. No se podría aislar la relación entre cada variable independiente y la dependiente. Si no se pueden aislar los efectos, se pueden confundir dichos efectos\n",
    "- Cuando las variables independientes están muy correlacionadas entre sí los cambios en una variable están asociados con cambios en otra variable y, por tanto, los coeficientes de regresión del modelo ya no van a medir el efecto de una variable independiente sobre la respuesta manteniendo constante, o sin variar, el resto de predictores. \n",
    "- Al contar con variables categóricas (la dependiente y varias dependientes) no podemos utilizar la matriz de correlación clásica que utiliza el coeficiente de correlación de Pearson ya que este tan sólo nos sirve para relación entre variables continuas.\n",
    "- Al haber varias características categóricas utilizamos la función associations de dython que utiliza el coeficiente de Pearson para continuas vs continuas, la razón de correlación para continuas vs categóricas y Cramer's V o Theil's U para categóricas vs categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utilizamos la librería dython para identificar las variables categóricas y numéricas\n",
    "\n",
    "from dython.nominal import identify_nominal_columns, identify_numeric_columns, correlation_ratio\n",
    "categorical_features=identify_nominal_columns(df_wins)\n",
    "continuous_features=identify_numeric_columns(df_wins)\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dython.nominal import associations\n",
    "complete_correlation= associations(df_wins, filename= 'complete_correlation.png', figsize=(20,20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_correlation= associations(df_wins, display_rows=['symboling'], filename= 'target_correlation.png', figsize=(20,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detallamos primero las variables independientes que más influencian a la objetivo (aunque ninguna tiene una alta correlación >= 0.7 o <= -0.7):\n",
    "- num_of_doors\n",
    "- normalized_losses\n",
    "- wheel_base\n",
    "- length\n",
    "- height\n",
    "- curb_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la matriz de correlación entre las variables continuas\n",
    "\n",
    "selected_column= df_wins[continuous_features]\n",
    "continuous_df = selected_column.copy()\n",
    "\n",
    "continuous_correlation= associations(continuous_df, filename= 'continuous_correlation.png', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos aquellas variables independientes continuas que pueden llegar a generar problemas al encontrarse altamente correlacionadas entre sí (problemas de multicolinealidad, redundancia, complejitud innecesaria del modelo):\n",
    "- wheel_base y length 0.88\n",
    "- wheel_base y width 0.8\n",
    "- wheel_base y curb_weight 0.78\n",
    "- lenght y width 0.85\n",
    "- length y curb_weight 0.88\n",
    "- length y engine_size 0.7\n",
    "- length y highway_mpg -0.7\n",
    "- length y price 0.7\n",
    "- width y curb_weight 0.87\n",
    "- width y engine_size 0.74\n",
    "- width y price 0.77\n",
    "- curb_weight y engine_size 0.87\n",
    "- curb_weight y horse_power 0.78\n",
    "- curb_weight y city_mpg -0.76\n",
    "- curb_weight y highway_mpg -0.8\n",
    "- curb_weight y price 0.85\n",
    "- engine_size y horse_power 0.84\n",
    "- engine_size y highway_mpg -0.7\n",
    "- engine_size y price 0.8\n",
    "- horse_power y city_mpg -0.85\n",
    "- horse_power y highway_mpg -0.83\n",
    "- horse_power y price 0.8\n",
    "- city_mpg y highway_mpg 0.97\n",
    "- city_mpg y price -0.74\n",
    "- highway_mpg y price -0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la matriz de correlación entre las variables categoricas\n",
    "\n",
    "selected_column= df_wins[categorical_features]\n",
    "categorical_df = selected_column.copy()\n",
    "\n",
    "categorical_correlation= associations(categorical_df, filename= 'categorical_correlation.png', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos aquellas variables independientes categóricas que pueden llegar a generar problemas al encontrarse altamente correlacionadas entre sí (redundacia y complejidad innecesaria del modelo):\n",
    "- make y engine_location 0.7\n",
    "- fuel_type y fuel_system 0.99\n",
    "- num_of_doors y body_style 0.75"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Ingeniería de características\n",
    "\n",
    "Vamos a eliminar y/o crear nuevas características que nos resulten útiles para la futura implementación de nuestros modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes variables podemos considerarlas como altamente correlacionadas entre sí (correlación >= .7 o correlación <= -.7):\n",
    "\n",
    "- city_mpg y highway_mpg: 0.97 (eliminamos city_mpg pues highway_mpg tiene una correlación ligeramente más alta con symboling)\n",
    "- length y curb_weight: 0.88 (eliminamos curb_weight pues length tiene una correlación más alta con symboling)\n",
    "- wheel_base y length: 0.88 (eliminamos length pues wheel_base tiene una correlación más alta con symboling)\n",
    "- engine_size y horse_power: 0.84 (eliminamos engine_size pues horse_power tiene una correlación más alta con symboling)\n",
    "- horse_power y highway_mpg: -0.83 (eliminamos horse_power pues highway_mpg tiene una correlación más alta con symboling)\n",
    "- wheel_base y width: 0.8 (eliminamos width pues wheel_base tiene una correlación más alta con symboling)\n",
    "- highway_mpg y price: -0.75 (eliminamos highway_mpg pues price tiene una correlación más alta con symboling)\n",
    "- fuel_type y fuel_system 0.99 (eliminamos fuel_type pues fuel_system tiene una correlación más alta con symboling)\n",
    "- num_of_doors y body_style: 0.75 (eliminamos body_style pues num_of_doors tiene una correlación más alta con symboling)\n",
    "- make y engine_location: 0.7 (eliminamos engine_location pues make tiene una correlación más alta con symboling)\n",
    "\n",
    "\n",
    "Puede resultar útil eliminar las variables que no se encuentren muy correlacionadas con symboling.\n",
    "- stroke\n",
    "- fuel_type\n",
    "- aspiration\n",
    "- drive_wheels\n",
    "- num_of_cylinders\n",
    "- compression_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlation_ratio(categories=df_wins['symboling'],measurements=df_wins['city_mpg']))\n",
    "print(correlation_ratio(categories=df_wins['symboling'],measurements=df_wins['highway_mpg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Transformer personalizado para realizar la eliminación de variables\n",
    "# a partir de sus correlaciones\n",
    "class Remove_highly_correlated_features(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,elims):\n",
    "        self.elims = elims\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        result = X.copy()\n",
    "        result = result.drop(columns=self.elims)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['city_mpg', 'curb_weight', 'length', 'engine_size', 'horse_power', 'wheel_base',\n",
    "        'highway_mpg', 'fuel_type', 'body_style', 'engine_location', 'stroke', 'fuel_type',\n",
    "        'aspiration', 'drive_wheels', 'num_of_cylinders', 'compression_ratio']\n",
    "\n",
    "df_elims = Remove_highly_correlated_features(cols).fit_transform(df_wins)\n",
    "df_elims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Transformer personalizado para dummificar las variables categóricas\n",
    "class Dummify(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        result = X.copy()\n",
    "        result = pd.get_dummies(result,columns=result.select_dtypes('object').drop(columns=['symboling']).columns,dtype=float)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elims_dummified = Dummify().fit_transform(df_elims)\n",
    "df_elims_dummified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo en cuenta que uno de los hallazgos importantes fue la alta correlación existente entre las variables independientes podemos aplicar PCA para reducir dimensiones y utilizar variables nuevas con mucha mayor información. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estandarizamos primero, pues de no hacerlo, al PCA basarse en distancias, se tendrán las variables nuevas fuertemente influenciadas por las variables originales de mayor escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos el estarizador\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_wins.select_dtypes('number'))\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=df_wins.select_dtypes('number').columns)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creamos el objeto PCA\n",
    "- Entrenamos y transformamos los datos originales\n",
    "- Los guardamos en un nuevo dataframe proyectado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "df_projected = pca.fit_transform(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos el dataset proyectado en las nuevas dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos cada uno de los componentes principales resultantes del proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos la varianza explicada de cada uno de los componentes principales (nos damos cuenta que los primeros componentes tienen mayor varianza explicada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miramos ahora el ratio de la varianza explicada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_exp=pca.explained_variance_ratio_\n",
    "cum_var_exp = np.cumsum(var_exp) \n",
    "var_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagramamos con el objetivo de identificar los componentes principales necesarios, aquellos que en conjunto nos expliquen un porcentaje considerable de la varianza (80%-95%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['PC' + str(x) for x in range(1, len(var_exp)+1)]\n",
    "plt.figure(figsize=(15, 7))\n",
    "bars = plt.bar(range(len(var_exp)), var_exp, alpha=0.3333, align='center', label='Varianza explicada por cada PC', \n",
    "         tick_label=labels, color = 'g',hatch='//')\n",
    "plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid',label='Varianza explicada acumulada')\n",
    "plt.ylabel('Porcentaje de varianza explicada')\n",
    "plt.xlabel('Componentes principales')\n",
    "plt.legend(loc='best')\n",
    "for i, bar in enumerate(bars):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n",
    "            f'{round(var_exp[i]*100,1)}%', ha='center', va='bottom', color='black', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos que entre los primeros 4 componentes principales tenemos el 80.4% de la información\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pca.explained_variance_ratio_[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora utilizamos los 4 primeros componentes, obteniendo un dataframe con las nuevas variables. En este caso particular, como nuestro objetivo es la predicción, no vamos a caracterizar las nuevas variables (esto sería útil si quisiésemos interpretar los resultados utilizando las variables generadas por el PCA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "pca_ = pca.fit_transform(df_scaled)\n",
    "pca_df = pd.DataFrame(pca_, columns=['pc1','pc2','pc3','pc4'],index=df.index)\n",
    "print('\\nFinal PCA:')\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Transformer personalizado para realizar PCA sobre las variables numéricas\n",
    "# Para dummificar las variables categóricas\n",
    "class PCA_transformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        result = X.copy()\n",
    "        cols = ['fuel_type', 'body_style', 'engine_location', 'stroke', 'fuel_type',\n",
    "        'aspiration', 'drive_wheels', 'num_of_cylinders', 'compression_ratio']\n",
    "        result = Remove_highly_correlated_features(cols).fit_transform(result)\n",
    "        df_scaled = scaler.fit_transform(result.select_dtypes('number'))\n",
    "        df_scaled = pd.DataFrame(df_scaled, columns=result.select_dtypes('number').columns)\n",
    "        pca = PCA(n_components=4)\n",
    "        pca_ = pca.fit_transform(df_scaled)\n",
    "        pca_df = pd.DataFrame(pca_, columns=['pc1','pc2','pc3','pc4'],index=df.index)\n",
    "        for col in result.select_dtypes('object'):\n",
    "            pca_df[col] = result[col] \n",
    "        pca_df = pd.get_dummies(pca_df,columns=pca_df.select_dtypes('object').drop(columns=['symboling']).columns,dtype=float)\n",
    "        return pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = PCA_transformer().fit_transform(df_wins)\n",
    "df_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos uso de un pipeline para dejar listos nuestros dataframes que posteriormente utilizaremos para entrenar nuestros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "elims = ['city_mpg', 'curb_weight', 'length', 'engine_size', 'horse_power', 'wheel_base',\n",
    "        'highway_mpg', 'fuel_type', 'body_style', 'engine_location', 'stroke', 'fuel_type',\n",
    "        'aspiration', 'drive_wheels', 'num_of_cylinders', 'compression_ratio']\n",
    "\n",
    "pipe_rem = Pipeline(steps = [('format target variable',Format_target()),\n",
    "                   ('format features',Format_variables()),\n",
    "                   ('impute null values',Imputer(cont,cat)),\n",
    "                   ('winsorize values',Winsorizer(wins_dict,cont_vars)),\n",
    "                   ('Remove highly correlated features',Remove_highly_correlated_features(elims)),\n",
    "                   ('Dummify categorical variables',Dummify())])\n",
    "\n",
    "pipe_pca_rem = Pipeline(steps = [('format target variable',Format_target()),\n",
    "                   ('format features',Format_variables()),\n",
    "                   ('impute null values',Imputer(cont,cat)),\n",
    "                   ('winsorize values',Winsorizer(wins_dict,cont_vars)),\n",
    "                   ('PCA',PCA_transformer())])\n",
    "\n",
    "\n",
    "df_pipeline_rem = pipe_rem.fit_transform(df)\n",
    "df_pipeline_pca_rem = pipe_pca_rem.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos una función que nos permite obetener el X y el y para el dataframe de carros\n",
    "\n",
    "def obtain_X_y(df):\n",
    "    return df.drop(columns=['symboling']),df[['symboling']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Protocolos de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos el holdout con un 30% para la prueba y 70% para el entrenamiento\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dict()\n",
    "y = dict()\n",
    "X_train = dict()\n",
    "y_train = dict()\n",
    "X_test = dict()\n",
    "y_test = dict()\n",
    "\n",
    "# Particionamos los datos para nuestra primer versión\n",
    "X['rem'], y['rem'] = obtain_X_y(df_pipeline_rem)\n",
    "\n",
    "X_train['rem'], X_test['rem'], y_train['rem'], y_test['rem'] = train_test_split(X['rem'],y['rem'],random_state=1234,test_size=0.3)\n",
    "print(X_train['rem'].shape)\n",
    "print(X_test['rem'].shape)\n",
    "print(y_train['rem'].shape)\n",
    "print(y_test['rem'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particionamos los datos para nuestra segunda versión\n",
    "X['pca_rem'], y['pca_rem'] = obtain_X_y(df_pipeline_pca_rem)\n",
    "\n",
    "X_train['pca_rem'], X_test['pca_rem'], y_train['pca_rem'], y_test['pca_rem'] = train_test_split(X['pca_rem'],y['pca_rem'],random_state=1234,test_size=0.3)\n",
    "print(X_train['pca_rem'].shape)\n",
    "print(X_test['pca_rem'].shape)\n",
    "print(y_train['pca_rem'].shape)\n",
    "print(y_test['pca_rem'].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Métricas de evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementamos una función que nos permita calcular las métricas de regresión para el conjunto de entrenamiento y de prueba (Accuracy, Kappa, Precision, Recall)\n",
    "\n",
    "Nos retornará el modelo entrenado y un diccionario con las principales métricas sobre el conjunto de entrenamiento y el de prueba\n",
    "- Accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
    "- Kappa = (OA - AC) / (1 - AC)\n",
    "- Precision = TP / (TP + FP) (valor positivo predicho, la proporción correcta de identificaciones positivas)\n",
    "- Recall = TP / (TP + FN) (la proporción de positivos que fueron identificados correctamente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, precision_score, recall_score\n",
    "\n",
    "def classification_metrics(model,X_train,X_test,y_train,y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "    kappa_train = cohen_kappa_score(y_train, y_pred_train)\n",
    "    kappa_test = cohen_kappa_score(y_test, y_pred_test)\n",
    "    prec_train = precision_score(y_train, y_pred_train,average=None,labels=['-1','0','1','2','3'])\n",
    "    prec_test = precision_score(y_test, y_pred_test,average=None,labels=['-1','0','1','2','3'])\n",
    "    recall_train = recall_score(y_train, y_pred_train,average=None,labels=['-1','0','1','2','3'])\n",
    "    recall_test = recall_score(y_test, y_pred_test,average=None,labels=['-1','0','1','2','3'])   \n",
    "    metrics = {\"Training Accuracy\": acc_train,\n",
    "               \"Test Accuracy\": acc_test,\n",
    "               \"Training Kappa\": kappa_train,\n",
    "               \"Test Kappa\": kappa_test,\n",
    "               \"Training Precision\": prec_train,\n",
    "               \"Test Precision\": prec_test,\n",
    "               \"Training recall\": recall_train,\n",
    "               \"Test recall\": recall_test    \n",
    "               }\n",
    "    for item in metrics.items():\n",
    "        print(item[0],\"=\",item[1])\n",
    "    return model,metrics\n",
    "    \n",
    "def kappa_metrics(model,X_train,X_test,y_train,y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    kappa_train = cohen_kappa_score(y_train, y_pred_train)\n",
    "    kappa_test = cohen_kappa_score(y_test, y_pred_test)\n",
    "    return kappa_train, kappa_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline a partir del dummy classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos la línea base a partir de un dummy classifier que utiliza como estrategia la moda (ya que estamos trabajando con una tarea de clasificación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "classification_metrics(DummyClassifier(strategy='most_frequent'),X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Accuracy = 30%, Test Accuracy = 37%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporte de métricas y matriz de confusión\n",
    "\n",
    "Vamos a definir una función para dibujar la matriz de confusión y el reporte de clasificación en donde podemos obtener las métricas de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Matriz de confusión',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Real')\n",
    "    plt.xlabel('Predicción')\n",
    "\n",
    "def report_and_conf_matrix(model,X_train,X_test,y_train,y_test):\n",
    "    md = model.fit(X_train,y_train)\n",
    "    y_pred_train = md.predict(X_train)\n",
    "    y_pred_test = md.predict(X_test)\n",
    "    cnf_matrix1 = confusion_matrix(y_train, y_pred_train)\n",
    "    cnf_matrix2 = confusion_matrix(y_test, y_pred_test)\n",
    "    np.set_printoptions(precision=2)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix1, classes=[\"-1\",\"0\",\"1\",\"2\",\"3\"],\n",
    "                      title='Matriz de confusión Entrenamiento')\n",
    "    print(\"Reporte Entrenamiento\")  \n",
    "    print(classification_report(y_train, y_pred_train, target_names=[\"-1\",\"0\",\"1\",\"2\",\"3\"]))\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix2, classes=[\"-1\",\"0\",\"1\",\"2\",\"3\"],\n",
    "                      title='Matriz de confusión Prueba')\n",
    "    print(\"Reporte Prueba\")  \n",
    "    print(classification_report(y_test, y_pred_test, target_names=[\"-1\",\"0\",\"1\",\"2\",\"3\"]))\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Área bajo la curva ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar el área bajo la curva ROC para evaluar nuestro modelo también. Con esta gráfica vamos a poder visualizar el desempeño del modelo entre la sensibilidad (recall - true positive rate) y la especificidad (true negative rate 1 - FPR), de manera que podamos establecer un balance entre ambos.\n",
    "- La curva ROC hace un plot entre el TPR y el FPR\n",
    "- Recall es la habilidad para identificar correctamente las observaciones que son positivas\n",
    "- Especificidad es la habilidad para indentificar correctamente las observaciones que son negativas\n",
    "- AUC ROC nos permite identificar que tan bien nuestro modelo puede distinguir entre las clases. \n",
    "\n",
    "Al tener nosotros un problema multi - clase utilizaremos una metodología uno vs el resto. Se tendrán tantas curvas como clases. Yellowbrick es muy útil y simple para esto\n",
    "\n",
    "- Utilizando las curvas individuales podemos ver que tan bien nuestro modelo trabaja para cada una de las categorías \n",
    "\n",
    "- La curva micro-average ROC agrega todas las instancias sobre todas las clases y nos brinda una sola curva para todas las predicciones (afectada por la moda).\n",
    "\n",
    "- La curva macro-average ROC trata clase de manera independiente y nos presenta una curva ROC que es el promedio de los valores para cada clase (cada clase tiene la misma importancia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ROCAUC\n",
    "\n",
    "def plot_ROC_curve(model, X_train, y_train, X_test, y_test):\n",
    "    visualizer = ROCAUC(model, encoder={'-1': 'riesgo negativo',\n",
    "                                        '0': 'neutral', \n",
    "                                        '1': 'riesgo 1', \n",
    "                                        '2': 'riesgo 2',\n",
    "                                        '3': 'riesgo 3'})                                    \n",
    "    visualizer.fit(X_train, y_train)\n",
    "    visualizer.score(X_test, y_test)\n",
    "    visualizer.show()\n",
    "    \n",
    "    return visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar el funcionamiento de la curva ROC y el área bajo ella utilizando uno de los modelos más sencillos para abordar tareas de clasificación, la regresión logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve(LogisticRegression(random_state=1234), X_train=X_train['rem'], y_train=y_train['rem'], X_test=X_test['rem'], y_test=y_test['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve(LogisticRegression(random_state=1234), X_train=X_train['pca_rem'], y_train=y_train['pca_rem'], X_test=X_test['pca_rem'], y_test=y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que el conjunto de datos al que se le ha realizado el PCA presenta mucho mejor desempeño. Más adelante ahondaremos más en esto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Implementación de modelos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1. Modelo de Regresión Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabajamos sobre el conjunto de datos con eliminación de variables altamente correlacionadas entre sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classification_metrics(LogisticRegression(random_state=1234),X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el mejor modelo con regresión logística\n",
    "\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "model,metrics = classification_metrics(LogisticRegressionCV(cv=5,random_state=1234),\n",
    "                                       X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])\n",
    "pickle.dump(model, open('models/logisticRegressionRem.pkl', 'wb'))\n",
    "pickle.dump(metrics, open('metrics/logisticRegressionRemMetrics.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro del contexto en el que nos encontramos trabajando, sería bueno entender que tipo de errores desearíamos que nuestro modelo evitara.\n",
    "\n",
    "Teniendo en cuenta que actuamos como una compañía aseguradora y queremos predecir el riesgo, quizás sería más importante al tomar como positiva la categoría más riesgosa, predecir que no es riesgosa y que sí lo sea (FN), es decir un recall más alto. Por otro lado al tomar como positiva la categoría menos riesgosa y predecir que no es riesgosa y que sí lo sea es un problema (FP), es decir una precision más alta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los hiperparámetros del mejor modelo de regresión logística\n",
    "\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_and_conf_matrix(LogisticRegressionCV(cv=5,random_state=1234),\n",
    "                       X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos el reporte y la matriz de confusión para nuestro mejor modelo de regresión logística con todas las variables.\n",
    "\n",
    "Con base en el reporte de entrenamiento podemos observar dos métricas importantes\n",
    "- Precision (para ver si tenemos un nivel alto de correctitud para la clase positiva). Una mayor precision disminuye los falsos positivos (en este caso desearíamos mayor precisión sobre las categorías de menos riesgo)\n",
    "- Recall (para ver qué porcentaje de las muestras puede el modelo identificar que pertenecen a la clase positiva). Un mayor recall indica que los falsos negativos disminuyen (en este caso desearíamos mayor sensibilidad sobre las categorías de mayor riesgo)\n",
    "\n",
    "La exactitud para la prueba de nuestro modelo es del 68%, mucho mejor que el presentado en el baseline que era del 37%\n",
    "\n",
    "La precisión para la clase menos riesgosa es de 80% \n",
    "\n",
    "La sensibilidad para la clase más riesgosa de 86% \n",
    "\n",
    "Ambas parecen adecuadas teniendo en cuenta el contexto del problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve(LogisticRegressionCV(cv=5,random_state=1234), \n",
    "               X_train=X_train['rem'], y_train=y_train['rem'], X_test=X_test['rem'], y_test=y_test['rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La curva micro-average arroja un valor adecuado de 0.85\n",
    "- La curva macro-average arroja un valor adecuado de 0.83\n",
    "- Aunque el modelo parece funcionar bien para las clases riesgo negativo y y riesgo 3, definitivamente no es tan correcto con la clase riesgo 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque previamente, en la etapa de ingeniería de características habíamos creado nuevas variables y eliminado otras, en esta etapa verificaremos la importancia de las variables independientes con respecto a la objetivo utilizando distintos métodos. \n",
    "\n",
    "Esto con el fin de hallar el mejor grupo de variables que sirvan como insumo para nuestros modelos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de características\n",
    "\n",
    "En este caso hacemos uso de SelectKBest que nos permite seleccionar las k mejores variables independientes teniendo en cuenta el f_classif. Construimos una función que a partir del conjunto de entrenamiento y el de prueba nos retorna los conjuntos ya transformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    " \n",
    "def select_features(X_train, y_train, X_test,n):\n",
    "  fs = SelectKBest(score_func=f_classif, k=n)\n",
    "  fs.fit(X_train, y_train)\n",
    "  X_train_fs = fs.transform(X_train)\n",
    "  X_test_fs = fs.transform(X_test)\n",
    "  return X_train_fs, X_test_fs, fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revizamos la importancia de las variables en el conjunto de datos sin PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haciendo uso de los conjuntos ya transformados podemos ver el score de cada variable\n",
    "# En este caso utilizamos todas las variables\n",
    "\n",
    "n = X_train['rem'].shape[1]\n",
    "\n",
    "X_train['rem_fs'], X_test['rem_fs'], fs = select_features(X_train['rem'], y_train['rem'], X_test['rem'],n)\n",
    "\n",
    "for i in range(len(fs.scores_)):\n",
    "    print('Feature %d: %f' % (i, fs.scores_[i]))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "ax.set(title='Ranking de características',xlabel='Características',ylabel='Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haciendo uso de un modelo de Random Forest Classifier hacemos un ranking de características\n",
    "# Ordenamos y graficamos\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=150,random_state=1234)\n",
    "rf.fit(X_train['rem'], y_train['rem'])\n",
    "sort = rf.feature_importances_.argsort()\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.barh(df_pipeline_rem.columns[1:][sort], rf.feature_importances_[sort])\n",
    "plt.xlabel(\"Importancia de características\")\n",
    "l = len(df_pipeline_rem.columns[1:])\n",
    "importants = df_pipeline_rem.columns[1:][sort][-5:]\n",
    "print(importants)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos el ranking de variables para el conjunto de datos con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=150,random_state=1234)\n",
    "rf.fit(X_train['pca_rem'], y_train['pca_rem'])\n",
    "sort = rf.feature_importances_.argsort()\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.barh(df_pipeline_rem.columns[1:][sort], rf.feature_importances_[sort])\n",
    "plt.xlabel(\"Importancia de características\")\n",
    "l = len(df_pipeline_pca_rem.columns[1:])\n",
    "importants = df_pipeline_pca_rem.columns[1:][sort][-5:]\n",
    "print(importants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos la cantidad de variables a seleccionar para el conjunto de datos sin PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "n = X_train['rem'].shape[1]\n",
    "results = [kappa_metrics(make_pipeline(SelectKBest(score_func=f_classif, k=i),\n",
    "                                                 StandardScaler(),\n",
    "                                                 RandomForestClassifier(n_estimators=150,random_state=1234)),\n",
    "            X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])[1] for i in range(1,n)]\n",
    "ax = sns.lineplot(x=range(1,n),\n",
    "             y=results)\n",
    "ax.set(title=\"Kappa vs K selected features\",ylabel='Kappa',xlabel='K selected features')\n",
    "kvals = [(col,i) for col,i in zip(results,range(1,n))]\n",
    "\n",
    "print(sorted(kvals, key=lambda x: x[0],reverse=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cantidad de variables a seleccionar para conjunto de datos sin PCA k = 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos la cantidad de variables a seleccionar del conjunto de datos con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X_train['pca_rem'].shape[1]\n",
    "results = [kappa_metrics(make_pipeline(SelectKBest(score_func=f_classif, k=i),\n",
    "                                                 StandardScaler(),\n",
    "                                                 RandomForestClassifier(n_estimators=150,random_state=1234)),\n",
    "            X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])[1] for i in range(1,n)]\n",
    "ax = sns.lineplot(x=range(1,n),\n",
    "             y=results)\n",
    "ax.set(title=\"Kappa vs K selected features\",ylabel='Kappa',xlabel='K selected features')\n",
    "kvals = [(col,i) for col,i in zip(results,range(1,n))]\n",
    "\n",
    "print(sorted(kvals, key=lambda x: x[0],reverse=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cantidad de variables a seleccionar para conjunto de datos con PCA k = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de regresión logística utilizando datos sin PCA y feature selection de 31 variables\n",
    "\n",
    "model,metrics = classification_metrics(make_pipeline(SelectKBest(score_func=f_classif, k=31),\n",
    "                                                 LogisticRegressionCV(cv=5,random_state=1234)),\n",
    "                                   X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])\n",
    "pickle.dump(model, open('models/logisticRegressionRemFS.pkl', 'wb'))\n",
    "pickle.dump(metrics, open('metrics/logisticRegressionRemFSMetrics.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = make_pipeline(SelectKBest(score_func=f_classif, k=31),\n",
    "                                                 LogisticRegressionCV(cv=5,random_state=1234))\n",
    "report_and_conf_matrix(pipeline_lr,X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La exactitud para la prueba de nuestro modelo es del 66%\n",
    "\n",
    "La precisión para la clase menos riesgosa es de 80% \n",
    "\n",
    "La sensibilidad para la clase más riesgosa de 71% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = make_pipeline(SelectKBest(score_func=f_classif, k=31),\n",
    "                                                 LogisticRegressionCV(cv=5,random_state=1234))\n",
    "plot_ROC_curve(pipeline_lr, X_train=X_train['rem'], y_train=y_train['rem'], X_test=X_test['rem'], y_test=y_test['rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La curva micro-average arroja un valor adecuado de 0.85\n",
    "- La curva macro-average arroja un valor adecuado de 0.82\n",
    "- Aunque el modelo parece funcionar bien para las clases riesgo negativo y y riesgo 3, definitivamente no es tan correcto con la clase riesgo 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de regresión logistica sobre conjunto de datos con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,metrics = classification_metrics(LogisticRegressionCV(cv=5,random_state=1234),\n",
    "                                       X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])\n",
    "pickle.dump(model, open('models/logisticRegressionPCARem.pkl', 'wb'))\n",
    "pickle.dump(metrics, open('metrics/logisticRegressionPCARemMetrics.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_and_conf_matrix(LogisticRegressionCV(cv=5,random_state=1234),\n",
    "                       X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La exactitud para la prueba de nuestro modelo es del 81%\n",
    "\n",
    "La precisión para la clase menos riesgosa es de 71% \n",
    "\n",
    "La sensibilidad para la clase más riesgosa de 86% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve(LogisticRegressionCV(cv=5,random_state=1234), \n",
    "               X_train=X_train['pca_rem'], y_train=y_train['pca_rem'], X_test=X_test['pca_rem'], y_test=y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La curva micro-average arroja un valor muy bueno de 0.95\n",
    "- La curva macro-average arroja un valor muy bueno de 0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de Regresión logística sobre conjunto de datos con PCA y feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de regresión logística utilizando datos con PCA y feature selection de 27 variables\n",
    "\n",
    "model,metrics = classification_metrics(make_pipeline(SelectKBest(score_func=f_classif, k=27),\n",
    "                                                 LogisticRegressionCV(cv=5,random_state=1234)),\n",
    "                                   X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])\n",
    "pickle.dump(model, open('models/logisticRegressionPCARemFS.pkl', 'wb'))\n",
    "pickle.dump(metrics, open('metrics/logisticRegressionPCARemFSMetrics.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = make_pipeline(SelectKBest(score_func=f_classif,k=27),\n",
    "                            LogisticRegressionCV(cv=5,random_state=1234))\n",
    "report_and_conf_matrix(pipeline_lr,X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La exactitud para la prueba de nuestro modelo es del 76%\n",
    "\n",
    "La precisión para la clase menos riesgosa es de 83% \n",
    "\n",
    "La sensibilidad para la clase más riesgosa de 86%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = make_pipeline(SelectKBest(score_func=f_classif,k=27),\n",
    "                            LogisticRegressionCV(cv=5,random_state=1234))\n",
    "plot_ROC_curve(pipeline_lr,X_train=X_train['pca_rem'], y_train=y_train['pca_rem'], X_test=X_test['pca_rem'], y_test=y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La curva micro-average arroja un valor muy bueno de 0.95\n",
    "- La curva macro-average arroja un valor muy bueno de 0.95"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Modelo de K vecinos más cercanos\n",
    "\n",
    "Para este modelo tan solamente incluimos las variables continuas, pues es un método basado en distancias. No tienen sentido utilizar las variables categóricas dummificadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalamos y estandarizamos para ver los resultados del modelo\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "print('Stadardization')\n",
    "knnReg_st = make_pipeline(StandardScaler(),KNeighborsClassifier())\n",
    "cols = ['normalized_losses', 'width', 'height', 'bore', 'peak_rpm', 'price']\n",
    "classification_metrics(knnReg_st,X_train['rem'][cols],X_test['rem'][cols],y_train['rem'],y_test['rem'])\n",
    "print()\n",
    "print('Normalization')\n",
    "knnReg_norm = make_pipeline(MinMaxScaler(),KNeighborsClassifier())\n",
    "classification_metrics(knnReg_norm,X_train['rem'][cols],X_test['rem'][cols],y_train['rem'],y_test['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos encontrar el mejor k para nuestro modelo\n",
    "# Utilizamos un pipeline para incluir la normalización y el clasificador knn \n",
    "\n",
    "cols = ['normalized_losses', 'width', 'height', 'bore', 'peak_rpm', 'price']\n",
    "for k in range(1,21,2):\n",
    "    print('Results for k =',k)\n",
    "    knnReg_st = make_pipeline(MinMaxScaler(),KNeighborsClassifier(n_neighbors=k))\n",
    "    classification_metrics(knnReg_st,X_train['rem'][cols],X_test['rem'][cols],y_train['rem'],y_test['rem'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos el kappa\n",
    "# Observamos la mejor respuesta con k=1\n",
    "\n",
    "cols = ['normalized_losses', 'width', 'height', 'bore', 'peak_rpm', 'price']\n",
    "knnReg_st = make_pipeline(MinMaxScaler(),KNeighborsClassifier(n_neighbors=k))\n",
    "results = [kappa_metrics(make_pipeline(MinMaxScaler(),KNeighborsClassifier(n_neighbors=i)),\n",
    "            X_train['rem'][cols],X_test['rem'][cols],y_train['rem'],y_test['rem'])[1] for i in range(1,21,2)]\n",
    "ax = sns.lineplot(x=range(1,21,2),\n",
    "             y=results)\n",
    "ax.set(title=\"Kappa vs K neighbors\",ylabel='Kappa',xlabel='K neighbors')\n",
    "kvals = [(col,i) for col,i in zip(results,range(1,21,2))]\n",
    "\n",
    "print(sorted(kvals, key=lambda x: x[0],reverse=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el mejor modelo utilizando k=1 sin PCA\n",
    "\n",
    "cols = ['normalized_losses', 'width', 'height', 'bore', 'peak_rpm', 'price']\n",
    "model,metrics = classification_metrics(make_pipeline(MinMaxScaler(),KNeighborsClassifier(n_neighbors=1)),\n",
    "                                   X_train['rem'][cols],X_test['rem'][cols],y_train['rem'],y_test['rem'])\n",
    "pickle.dump(model, open('models/KnnClassifierRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/KnnClassifierRemMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_and_conf_matrix(make_pipeline(MinMaxScaler(),KNeighborsClassifier(n_neighbors=1)),\n",
    "                       X_train['rem'][cols],X_test['rem'][cols],y_train['rem'],y_test['rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La exactitud para la prueba de nuestro modelo es del 73%\n",
    "\n",
    "La precisión para la clase menos riesgosa es de 80% \n",
    "\n",
    "La sensibilidad para la clase más riesgosa de 71%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve(make_pipeline(MinMaxScaler(),KNeighborsClassifier(n_neighbors=1)),\n",
    "                X_train=X_train['rem'][cols], y_train=y_train['rem'],X_test=X_test['rem'][cols], y_test=y_test['rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos peores resultados que con la regresión logística, las AUC micro y macro son de 0.83 y 0.8 respectivamente. Aunque aceptables, peores que los obtenidos previamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora trabajamos con el dataset con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos encontrar el mejor k para nuestro modelo\n",
    "# Utilizamos un pipeline para incluir la normalización y el clasificador knn \n",
    "\n",
    "cols = ['pc1','pc2','pc3','pc4']\n",
    "for k in range(1,21,2):\n",
    "    print('Results for k =',k)\n",
    "    knnReg_st = make_pipeline(MinMaxScaler(),KNeighborsClassifier(n_neighbors=k))\n",
    "    classification_metrics(knnReg_st,X_train['pca_rem'][cols],X_test['pca_rem'][cols],y_train['pca_rem'],y_test['pca_rem'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos el kappa\n",
    "# Observamos la mejor respuesta con k=1\n",
    "\n",
    "cols = ['pc1','pc2','pc3','pc4']\n",
    "knnReg_st = make_pipeline(MinMaxScaler(),KNeighborsClassifier(n_neighbors=k))\n",
    "results = [kappa_metrics(make_pipeline(MinMaxScaler(),KNeighborsClassifier(n_neighbors=i)),\n",
    "            X_train['pca_rem'][cols],X_test['pca_rem'][cols],y_train['pca_rem'],y_test['pca_rem'])[1] for i in range(1,21,2)]\n",
    "ax = sns.lineplot(x=range(1,21,2),\n",
    "             y=results)\n",
    "ax.set(title=\"Kappa vs K neighbors\",ylabel='Kappa',xlabel='K neighbors')\n",
    "kvals = [(col,i) for col,i in zip(results,range(1,21,2))]\n",
    "\n",
    "print(sorted(kvals, key=lambda x: x[0],reverse=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el mejor modelo utilizando k=1 sin PCA\n",
    "\n",
    "cols = ['pc1','pc2','pc3','pc4']\n",
    "model,metrics = classification_metrics(make_pipeline(MinMaxScaler(),KNeighborsClassifier(n_neighbors=1)),\n",
    "                                   X_train['pca_rem'][cols],X_test['pca_rem'][cols],y_train['pca_rem'],y_test['pca_rem'])\n",
    "pickle.dump(model, open('models/KnnClassifierPCARem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/KnnClassifierPCARemMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_and_conf_matrix(make_pipeline(MinMaxScaler(),KNeighborsClassifier(n_neighbors=1)),\n",
    "                       X_train['pca_rem'][cols],X_test['pca_rem'][cols],y_train['pca_rem'],y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve(make_pipeline(MinMaxScaler(),KNeighborsClassifier(n_neighbors=1)),\n",
    "                X_train=X_train['pca_rem'][cols], y_train=y_train['pca_rem'],X_test=X_test['pca_rem'][cols], y_test=y_test['pca_rem'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. Modelo Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos primero el conjunto de datos sin PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "classification_metrics(GaussianNB(),X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = GaussianNB()\n",
    "\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "gs_NB = GridSearchCV(estimator=nb_classifier, \n",
    "                 param_grid=params_NB, \n",
    "                 cv=5, \n",
    "                 verbose=1, \n",
    "                 scoring='accuracy') \n",
    "gs_NB.fit(X_train['rem'], y_train['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejores hiperparámetros\n",
    "\n",
    "gs_NB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_NB.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el mejor modelo utilizando el dataset sin PCA\n",
    "\n",
    "model,metrics = classification_metrics(GaussianNB(var_smoothing=1.232846739442066e-08),\n",
    "                                   X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])\n",
    "pickle.dump(model, open('models/NBClassifierRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/NBClassifierRemMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_and_conf_matrix(GaussianNB(var_smoothing=1.232846739442066e-08),\n",
    "                       X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve(GaussianNB(var_smoothing=1.232846739442066e-08),\n",
    "                X_train=X_train['rem'], y_train=y_train['rem'],X_test=X_test['rem'], y_test=y_test['rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabajamos con el conjunto de datos sin PCA pero con feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = GaussianNB()\n",
    "\n",
    "X_t = SelectKBest(score_func=f_classif,k=31).fit_transform(X_train['rem'], y_train['rem'])\n",
    "\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "gs_NB = GridSearchCV(estimator=nb_classifier, \n",
    "                 param_grid=params_NB, \n",
    "                 cv=5, \n",
    "                 verbose=1, \n",
    "                 scoring='accuracy') \n",
    "gs_NB.fit(X_t, y_train['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_NB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_NB.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el mejor modelo sin PCA utilizando feature selection\n",
    "\n",
    "pipeline_nb = make_pipeline(SelectKBest(score_func=f_classif,k=31),\n",
    "                            GaussianNB(var_smoothing=6.579332246575682e-09))\n",
    "\n",
    "model,metrics = classification_metrics(pipeline_nb,\n",
    "                                   X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])\n",
    "pickle.dump(model, open('models/NBClassifierRemFS.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/NBClassifierRemFSMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_nb = make_pipeline(SelectKBest(score_func=f_classif,k=31),\n",
    "                            GaussianNB(var_smoothing=6.579332246575682e-09))\n",
    "report_and_conf_matrix(pipeline_nb,X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_nb = make_pipeline(SelectKBest(score_func=f_classif,k=31),\n",
    "                            GaussianNB(var_smoothing=6.579332246575682e-09))\n",
    "plot_ROC_curve(pipeline_nb,X_train=X_train['rem'], y_train=y_train['rem'],X_test=X_test['rem'], y_test=y_test['rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos el conjunto de datos con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = GaussianNB()\n",
    "\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "gs_NB = GridSearchCV(estimator=nb_classifier, \n",
    "                 param_grid=params_NB, \n",
    "                 cv=5, \n",
    "                 verbose=1, \n",
    "                 scoring='accuracy') \n",
    "gs_NB.fit(X_train['pca_rem'], y_train['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_NB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_NB.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el mejor modelo utilizando el dataset con PCA\n",
    "\n",
    "model,metrics = classification_metrics(GaussianNB(var_smoothing=0.0657933224657568),\n",
    "                                   X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])\n",
    "pickle.dump(model, open('models/NBClassifierPCARem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/NBClassifierPCARemMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos el conjunto de datos con PCA y con feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = GaussianNB()\n",
    "\n",
    "X_t = SelectKBest(score_func=f_classif,k=27).fit_transform(X_train['pca_rem'], y_train['pca_rem'])\n",
    "\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "gs_NB = GridSearchCV(estimator=nb_classifier, \n",
    "                 param_grid=params_NB, \n",
    "                 cv=5, \n",
    "                 verbose=1, \n",
    "                 scoring='accuracy') \n",
    "gs_NB.fit(X_t, y_train['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_NB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_NB.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el mejor modelo sin PCA utilizando feature selection\n",
    "\n",
    "pipeline_nb = make_pipeline(SelectKBest(score_func=f_classif,k=27),\n",
    "                            GaussianNB(var_smoothing=0.03511191734215131))\n",
    "\n",
    "model,metrics = classification_metrics(pipeline_nb,\n",
    "                                   X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])\n",
    "pickle.dump(model, open('models/NBClassifierPCARemFS.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/NBClassifierPCARemFSMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_nb = make_pipeline(SelectKBest(score_func=f_classif,k=27),\n",
    "                            GaussianNB(var_smoothing=0.03511191734215131))\n",
    "report_and_conf_matrix(pipeline_nb,\n",
    "                X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_nb = make_pipeline(SelectKBest(score_func=f_classif,k=27),\n",
    "                            GaussianNB(var_smoothing=0.03511191734215131))\n",
    "plot_ROC_curve(pipeline_nb,X_train=X_train['pca_rem'], y_train=y_train['pca_rem'], X_test=X_test['pca_rem'], y_test=y_test['pca_rem'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. Modelo de Árbol de Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjunto de datos sin PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos un árbol de clasificación\n",
    "# Verificamos sus métricas\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "classification_metrics(DecisionTreeClassifier(random_state=1234),X_train['rem'],X_test['rem'],y_train['pca_rem'],y_test['pca_rem'])\n",
    "\n",
    "tree_reg = DecisionTreeClassifier(random_state=1234).fit(X_train['rem'],y_train['rem'])\n",
    "print(tree_reg)\n",
    "\n",
    "# Diagramamos los primeros tres niveles del árbol resultante\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plot_tree(tree_reg,filled=False,fontsize=14,max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a realizar un ajuste de hiperparámetros para encontrar el mejor modelo de árbol de clasificación\n",
    "# Establecemos la grilla de parámetros que verificar\n",
    "# Haciendo uso de estos parámetros y una validación cruzada de 3 doblajes buscamos el mejor modelo\n",
    "# Tiempo aprox = 9s \n",
    "\n",
    "parameters={\"splitter\":[\"best\",\"random\"],\n",
    "            \"max_depth\" : [1,3,5,7,9,11],\n",
    "           \"min_samples_leaf\":[1,2,3,4,],\n",
    "           \"max_leaf_nodes\":[None,10,20,30] }\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "kappa_scorer = make_scorer(cohen_kappa_score)\n",
    "tuning_model=GridSearchCV(DecisionTreeClassifier(random_state=1234),\n",
    "                          param_grid=parameters,scoring=kappa_scorer,\n",
    "                          cv=3,verbose=3)\n",
    "\n",
    "tuning_model.fit(X_train['rem'],y_train['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los híperparámetros del mejor modelo\n",
    "\n",
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El mejor modelo de árbol de clasificación sin PCA\n",
    "\n",
    "\n",
    "tree_clsfr = DecisionTreeClassifier(random_state=1234,max_depth=9,\n",
    "                                         max_leaf_nodes=30,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         splitter='best')\n",
    "\n",
    "model,metrics = classification_metrics(tree_clsfr,X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])\n",
    "pickle.dump(model, open('models/DecissionTreeClassifierRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/DecisionTreeClassifierRemMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_and_conf_matrix(DecisionTreeClassifier(random_state=1234,max_depth=11,\n",
    "                                         max_leaf_nodes=30,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         splitter='best'),\n",
    "                       X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve(DecisionTreeClassifier(random_state=1234,max_depth=11,\n",
    "                                         max_leaf_nodes=30,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         splitter='best'),\n",
    "                X_train=X_train['rem'], y_train=y_train['rem'],X_test=X_test['rem'], y_test=y_test['rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjunto de datos sin PCA y con feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo aproximado: 8s\n",
    "\n",
    "parameters={\"splitter\":[\"best\",\"random\"],\n",
    "            \"max_depth\" : [1,3,5,7,9,11],\n",
    "           \"min_samples_leaf\":[1,2,3,4,],\n",
    "           \"max_leaf_nodes\":[None,10,20,30] }\n",
    "\n",
    "X_t = SelectKBest(score_func=f_classif,k=31).fit_transform(X_train['rem'],y_train['rem'])\n",
    "\n",
    "tuning_model=GridSearchCV(DecisionTreeClassifier(random_state=1234),param_grid=parameters,scoring=kappa_scorer,\n",
    "                          cv=3,verbose=3)\n",
    "\n",
    "tuning_model.fit(X_t,y_train['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El mejor modelo de árbol de regresión sin PCA\n",
    "# Con feature selection\n",
    "\n",
    "pipeline_dt = make_pipeline(SelectKBest(score_func=f_classif,k=31),\n",
    "                            DecisionTreeClassifier(random_state=1234,max_depth=9,\n",
    "                                         max_leaf_nodes=None,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         splitter='best'))\n",
    "\n",
    "model,metrics = classification_metrics(pipeline_dt,X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])\n",
    "pickle.dump(model, open('models/DecissionTreeClassifierRemFS.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/DecisionTreeClassifierRemFSMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dt = make_pipeline(SelectKBest(score_func=f_classif,k=31),\n",
    "                            DecisionTreeClassifier(random_state=1234,max_depth=9,\n",
    "                                         max_leaf_nodes=None,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         splitter='best'))\n",
    "report_and_conf_matrix(pipeline_dt,X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dt = make_pipeline(SelectKBest(score_func=f_classif,k=31),\n",
    "                            DecisionTreeClassifier(random_state=1234,max_depth=9,\n",
    "                                         max_leaf_nodes=None,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         splitter='best'))\n",
    "plot_ROC_curve(pipeline_dt,X_train=X_train['rem'], y_train=y_train['rem'], X_test=X_test['rem'], y_test=y_test['rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo con conjunto de datos con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo aproximado: 11s\n",
    "\n",
    "parameters={\"splitter\":[\"best\",\"random\"],\n",
    "            \"max_depth\" : [1,3,5,7,9,11],\n",
    "           \"min_samples_leaf\":[1,2,3,4,],\n",
    "           \"max_leaf_nodes\":[None,10,20,30] }\n",
    "\n",
    "tuning_model=GridSearchCV(DecisionTreeClassifier(random_state=1234),\n",
    "                          param_grid=parameters,scoring=kappa_scorer,\n",
    "                          cv=3,verbose=3)\n",
    "\n",
    "tuning_model.fit(X_train['pca_rem'],y_train['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El mejor modelo de árbol de clasificación con PCA\n",
    "\n",
    "\n",
    "tree_clsfr = DecisionTreeClassifier(random_state=1234,max_depth=7,\n",
    "                                         max_leaf_nodes=None,\n",
    "                                         min_samples_leaf=4,\n",
    "                                         splitter='best')\n",
    "\n",
    "model,metrics = classification_metrics(tree_clsfr,X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])\n",
    "pickle.dump(model, open('models/DecissionTreeClassifierPCARem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/DecisionTreeClassifierPCARemMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_and_conf_matrix(DecisionTreeClassifier(random_state=1234,max_depth=7,\n",
    "                                         max_leaf_nodes=None,\n",
    "                                         min_samples_leaf=4,\n",
    "                                         splitter='best'),\n",
    "                       X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve(DecisionTreeClassifier(random_state=1234,max_depth=7,\n",
    "                                         max_leaf_nodes=None,\n",
    "                                         min_samples_leaf=4,\n",
    "                                         splitter='best'),\n",
    "                X_train=X_train['pca_rem'], y_train=y_train['pca_rem'],X_test=X_test['pca_rem'], y_test=y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo con conjunto de datos con PCA y feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo aproximado: 9s\n",
    "\n",
    "parameters={\"splitter\":[\"best\",\"random\"],\n",
    "            \"max_depth\" : [1,3,5,7,9,11],\n",
    "           \"min_samples_leaf\":[1,2,3,4,],\n",
    "           \"max_leaf_nodes\":[None,10,20,30] }\n",
    "\n",
    "X_t = SelectKBest(score_func=f_classif,k=27).fit_transform(X_train['pca_rem'],y_train['pca_rem'])\n",
    "\n",
    "tuning_model=GridSearchCV(DecisionTreeClassifier(random_state=1234),param_grid=parameters,scoring=kappa_scorer,\n",
    "                          cv=3,verbose=3)\n",
    "\n",
    "tuning_model.fit(X_t,y_train['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El mejor modelo de árbol de regresión con PCA\n",
    "# Con feature selection\n",
    "\n",
    "pipeline_dt = make_pipeline(SelectKBest(score_func=f_classif,k=27),\n",
    "                            DecisionTreeClassifier(random_state=1234,max_depth=7,\n",
    "                                         max_leaf_nodes=10,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         splitter='best'))\n",
    "\n",
    "model,metrics = classification_metrics(pipeline_dt,X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])\n",
    "pickle.dump(model, open('models/DecissionTreeClassifierPCARemFS.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/DecisionTreeClassifierPCARemFSMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dt = make_pipeline(SelectKBest(score_func=f_classif,k=27),\n",
    "                            DecisionTreeClassifier(random_state=1234,max_depth=7,\n",
    "                                         max_leaf_nodes=10,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         splitter='best'))\n",
    "report_and_conf_matrix(pipeline_dt,X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dt = make_pipeline(SelectKBest(score_func=f_classif,k=31),\n",
    "                            DecisionTreeClassifier(random_state=1234,max_depth=7,\n",
    "                                         max_leaf_nodes=10,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         splitter='best'))\n",
    "plot_ROC_curve(pipeline_dt,X_train=X_train['pca_rem'], y_train=y_train['pca_rem'], X_test=X_test['pca_rem'], y_test=y_test['pca_rem'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6. Modelo de Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjunto de datos sin PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=1234)\n",
    "\n",
    "classification_metrics(rf,X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])\n",
    "\n",
    "rf.fit(X_train['rem'],y_train['rem'])\n",
    "print(rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo aproximado: 2m\n",
    "\n",
    "params = {'max_depth': [None, 10, 20],\n",
    "          'min_samples_leaf': [1, 2, 3],\n",
    "          'min_samples_split': [2, 5],\n",
    "          'n_estimators': [100, 200, 400]}\n",
    "\n",
    "tuning_model=GridSearchCV(RandomForestClassifier(random_state=1234),param_grid=params,scoring=kappa_scorer,cv=5,verbose=3)\n",
    "\n",
    "tuning_model.fit(X_train['rem'],y_train['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El mejor modelo de Random Forest\n",
    "# Sin PCA\n",
    "\n",
    "\n",
    "rf_clsfr = RandomForestClassifier(random_state=1234,max_depth=None,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=5,\n",
    "                                         n_estimators=100)\n",
    "\n",
    "model,metrics = classification_metrics(rf_clsfr,X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])\n",
    "pickle.dump(model, open('models/RandomForestClassifierRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/RandomForestClassifierRemMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_and_conf_matrix(RandomForestClassifier(random_state=1234,max_depth=None,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=5,\n",
    "                                         n_estimators=100),\n",
    "                       X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve(RandomForestClassifier(random_state=1234,max_depth=None,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=5,\n",
    "                                         n_estimators=100),\n",
    "                X_train=X_train['rem'], y_train=y_train['rem'],X_test=X_test['rem'], y_test=y_test['rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo sin PCA pero con feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo aproximado: 3m\n",
    "\n",
    "params = {'max_depth': [None, 10, 20],\n",
    "          'min_samples_leaf': [1, 2, 3],\n",
    "          'min_samples_split': [2, 5],\n",
    "          'n_estimators': [100, 200, 400]}\n",
    "\n",
    "X_t = SelectKBest(score_func=f_classif,k=31).fit_transform(X_train['rem'],y_train['rem'])\n",
    "\n",
    "tuning_model=GridSearchCV(RandomForestClassifier(random_state=1234),param_grid=params,scoring=kappa_scorer,cv=5,verbose=3)\n",
    "\n",
    "tuning_model.fit(X_t,y_train['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El mejor modelo de random forest sin PCA\n",
    "# Con feature selection\n",
    "\n",
    "rf_clsfr = RandomForestClassifier(random_state=1234,max_depth=None,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=2,\n",
    "                                         n_estimators=200)\n",
    "\n",
    "X_t = SelectKBest(score_func=f_classif,k=31).fit_transform(X_train['rem'],y_train['rem'])\n",
    "\n",
    "model,metrics = classification_metrics(rf_clsfr,X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])\n",
    "pickle.dump(model, open('models/RandomForestClassifierRemFS.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/RandomForestClassifierRemFSMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf = make_pipeline(SelectKBest(score_func=f_classif,k=31),\n",
    "                          RandomForestClassifier(random_state=1234,max_depth=None,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=2,\n",
    "                                         n_estimators=200))\n",
    "report_and_conf_matrix(pipeline_rf,X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf = make_pipeline(SelectKBest(score_func=f_classif,k=31),\n",
    "                          RandomForestClassifier(random_state=1234,max_depth=None,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=2,\n",
    "                                         n_estimators=200))\n",
    "plot_ROC_curve(pipeline_rf,X_train=X_train['rem'], y_train=y_train['rem'], X_test=X_test['rem'], y_test=y_test['rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo con conjunto de datos con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo aproximado: 2m\n",
    "\n",
    "params = {'max_depth': [None, 10, 20],\n",
    "          'min_samples_leaf': [1, 2, 3],\n",
    "          'min_samples_split': [2, 5],\n",
    "          'n_estimators': [100, 200, 400]}\n",
    "\n",
    "tuning_model=GridSearchCV(RandomForestClassifier(random_state=1234),param_grid=params,scoring=kappa_scorer,cv=5,verbose=3)\n",
    "\n",
    "tuning_model.fit(X_train['pca_rem'],y_train['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El mejor modelo de Random Forest\n",
    "# Con PCA\n",
    "\n",
    "\n",
    "rf_clsfr = RandomForestClassifier(random_state=1234,max_depth=None,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=5,\n",
    "                                         n_estimators=400)\n",
    "\n",
    "model,metrics = classification_metrics(rf_clsfr,X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])\n",
    "pickle.dump(model, open('models/RandomForestClassifierPCARem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/RandomForestClassifierPCARemMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_and_conf_matrix(RandomForestClassifier(random_state=1234,max_depth=None,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=5,\n",
    "                                         n_estimators=400),\n",
    "                       X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve(RandomForestClassifier(random_state=1234,max_depth=None,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=5,\n",
    "                                         n_estimators=400),\n",
    "                X_train=X_train['pca_rem'], y_train=y_train['pca_rem'],X_test=X_test['pca_rem'], y_test=y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo con PCA y feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo aproximado: 3m\n",
    "\n",
    "params = {'max_depth': [None, 10, 20],\n",
    "          'min_samples_leaf': [1, 2, 3],\n",
    "          'min_samples_split': [2, 5],\n",
    "          'n_estimators': [100, 200, 400]}\n",
    "\n",
    "X_t = SelectKBest(score_func=f_classif,k=27).fit_transform(X_train['pca_rem'],y_train['pca_rem'])\n",
    "\n",
    "tuning_model=GridSearchCV(RandomForestClassifier(random_state=1234),param_grid=params,scoring=kappa_scorer,cv=5,verbose=3)\n",
    "\n",
    "tuning_model.fit(X_t,y_train['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El mejor modelo de random forest con PCA\n",
    "# Con feature selection\n",
    "\n",
    "rf_clsfr = RandomForestClassifier(random_state=1234,max_depth=10,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=5,\n",
    "                                         n_estimators=100)\n",
    "\n",
    "X_t = SelectKBest(score_func=f_classif,k=27).fit_transform(X_train['pca_rem'],y_train['pca_rem'])\n",
    "\n",
    "model,metrics = classification_metrics(rf_clsfr,X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])\n",
    "pickle.dump(model, open('models/RandomForestClassifierPCARemFS.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/RandomForestClassifierPCARemFSMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf = make_pipeline(SelectKBest(score_func=f_classif,k=27),\n",
    "                          RandomForestClassifier(random_state=1234,max_depth=10,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=5,\n",
    "                                         n_estimators=100))\n",
    "report_and_conf_matrix(pipeline_rf,X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf = make_pipeline(SelectKBest(score_func=f_classif,k=31),\n",
    "                          RandomForestClassifier(random_state=1234,max_depth=10,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=5,\n",
    "                                         n_estimators=100))\n",
    "plot_ROC_curve(pipeline_rf,X_train=X_train['pca_rem'], y_train=y_train['pca_rem'], X_test=X_test['pca_rem'], y_test=y_test['pca_rem'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7. Modelo de Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo con datos sin PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=1234)\n",
    "\n",
    "classification_metrics(gb,X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])\n",
    "\n",
    "gb.fit(X_train['rem'],y_train['rem'])\n",
    "print(gb.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tiempo aproximado: 15m\n",
    "\n",
    "params = {'learning_rate': [0.01,0.03],\n",
    "                  'subsample'    : [0.5, 0.2],\n",
    "                  'n_estimators' : [500,1000],\n",
    "                  'max_depth'    : [4,6,8]    \n",
    "            }\n",
    "\n",
    "tuning_model=GridSearchCV(GradientBoostingClassifier(random_state=1234),\n",
    "                          param_grid=params,scoring=kappa_scorer,cv=5,verbose=3)\n",
    "tuning_model.fit(X_train['rem'],y_train['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El mejor modelo de Gradient Boosting\n",
    "# Sin PCA\n",
    " \n",
    "\n",
    "gb_clsfr = GradientBoostingClassifier(random_state=1234,learning_rate=0.01, \n",
    "                                      max_depth=6, n_estimators=1000,subsample=0.5)\n",
    "\n",
    "model,metrics = classification_metrics(gb_clsfr,X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])\n",
    "pickle.dump(model, open('models/GradientBoostingClassifierRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/GradientBoostingClassifierRemMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_and_conf_matrix(GradientBoostingClassifier(random_state=1234,learning_rate=0.01, \n",
    "                                      max_depth=6, n_estimators=1000,subsample=0.5),\n",
    "                       X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve(GradientBoostingClassifier(random_state=1234,learning_rate=0.01, \n",
    "                                      max_depth=6, n_estimators=1000,subsample=0.5),\n",
    "                X_train=X_train['rem'], y_train=y_train['rem'],X_test=X_test['rem'], y_test=y_test['rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjunto de datos sin PCA y con feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tiempo aproximado: 13m\n",
    "\n",
    "params = {'learning_rate': [0.01,0.03],\n",
    "                  'subsample'    : [0.5, 0.2],\n",
    "                  'n_estimators' : [500,1000],\n",
    "                  'max_depth'    : [4,6,8]    \n",
    "            }\n",
    "\n",
    "X_t = SelectKBest(score_func=f_classif,k=31).fit_transform(X_train['rem'],y_train['rem'])\n",
    "\n",
    "tuning_model=GridSearchCV(GradientBoostingClassifier(random_state=1234),\n",
    "                          param_grid=params,scoring=kappa_scorer,cv=5,verbose=3)\n",
    "tuning_model.fit(X_t,y_train['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El mejor modelo de Gradient Boosting sin PCA\n",
    "# Con feature selection\n",
    "\n",
    "\n",
    "pipeline_gb = make_pipeline(SelectKBest(score_func=f_classif,k=31),\n",
    "                           GradientBoostingClassifier(random_state=1234,learning_rate=0.01, \n",
    "                                      max_depth=6, n_estimators=1000,subsample=0.5))\n",
    "\n",
    "model,metrics = classification_metrics(pipeline_gb,X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])\n",
    "pickle.dump(model, open('models/GradientBoostingClassifierRemFS.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/GradientBoostingClassifierRemFSMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_gb = make_pipeline(SelectKBest(score_func=f_classif,k=31),\n",
    "                           GradientBoostingClassifier(random_state=1234,learning_rate=0.01, \n",
    "                                      max_depth=6, n_estimators=1000,subsample=0.5))\n",
    "report_and_conf_matrix(pipeline_gb,X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_gb = make_pipeline(SelectKBest(score_func=f_classif,k=31),\n",
    "                           GradientBoostingClassifier(random_state=1234,learning_rate=0.01, \n",
    "                                      max_depth=6, n_estimators=1000,subsample=0.5))\n",
    "plot_ROC_curve(pipeline_gb,X_train=X_train['rem'], y_train=y_train['rem'], X_test=X_test['rem'], y_test=y_test['rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tiempo aproximado: 17m\n",
    "\n",
    "params = {'learning_rate': [0.01,0.03],\n",
    "                  'subsample'    : [0.5, 0.2],\n",
    "                  'n_estimators' : [500,1000],\n",
    "                  'max_depth'    : [4,6,8]    \n",
    "            }\n",
    "\n",
    "tuning_model=GridSearchCV(GradientBoostingClassifier(random_state=1234),\n",
    "                          param_grid=params,scoring=kappa_scorer,cv=5,verbose=3)\n",
    "tuning_model.fit(X_train['pca_rem'],y_train['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El mejor modelo de Gradient Boosting\n",
    "# Con PCA\n",
    " \n",
    "\n",
    "gb_clsfr = GradientBoostingClassifier(random_state=1234,learning_rate=0.03, \n",
    "                                      max_depth=4, n_estimators=1000,subsample=0.5)\n",
    "\n",
    "model,metrics = classification_metrics(gb_clsfr,X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])\n",
    "pickle.dump(model, open('models/GradientBoostingClassifierPCARem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/GradientBoostingClassifierPCARemMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_and_conf_matrix(GradientBoostingClassifier(random_state=1234,learning_rate=0.03, \n",
    "                                      max_depth=4, n_estimators=1000,subsample=0.5),\n",
    "                       X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve(GradientBoostingClassifier(random_state=1234,learning_rate=0.03, \n",
    "                                      max_depth=4, n_estimators=1000,subsample=0.5),\n",
    "                X_train=X_train['pca_rem'], y_train=y_train['pca_rem'],X_test=X_test['pca_rem'], y_test=y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo con PCA y feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tiempo aproximado: 70m\n",
    "\n",
    "params = {'learning_rate': [0.01,0.03],\n",
    "                  'subsample'    : [0.5, 0.2],\n",
    "                  'n_estimators' : [500,1000],\n",
    "                  'max_depth'    : [4,6,8]    \n",
    "            }\n",
    "\n",
    "X_t = SelectKBest(score_func=f_classif,k=31).fit_transform(X_train['pca_rem'],y_train['pca_rem'])\n",
    "\n",
    "tuning_model=GridSearchCV(GradientBoostingClassifier(random_state=1234),\n",
    "                          param_grid=params,scoring=kappa_scorer,cv=5,verbose=3)\n",
    "tuning_model.fit(X_t,y_train['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El mejor modelo de Gradient Boosting\n",
    "# Con PCA y feature selection\n",
    " \n",
    "X_t = SelectKBest(score_func=f_classif,k=27).fit_transform(X_train['pca_rem'],y_train['pca_rem'])\n",
    "\n",
    "gb_clsfr = GradientBoostingClassifier(random_state=1234,learning_rate=0.03, \n",
    "                                      max_depth=4, n_estimators=500,subsample=0.5)\n",
    "\n",
    "model,metrics = classification_metrics(gb_clsfr,X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])\n",
    "pickle.dump(model, open('models/GradientBoostingClassifierPCARemFS.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/GradientBoostingClassifierPCARemFSMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_gb = make_pipeline(SelectKBest(score_func=f_classif,k=27),\n",
    "                           GradientBoostingClassifier(random_state=1234,learning_rate=0.03, \n",
    "                                      max_depth=4, n_estimators=500,subsample=0.5))\n",
    "report_and_conf_matrix(pipeline_gb,X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_gb = make_pipeline(SelectKBest(score_func=f_classif,k=27),\n",
    "                           GradientBoostingClassifier(random_state=1234,learning_rate=0.03, \n",
    "                                      max_depth=4, n_estimators=500,subsample=0.5))\n",
    "plot_ROC_curve(pipeline_gb,X_train=X_train['pca_rem'], y_train=y_train['pca_rem'],X_test=X_test['pca_rem'], y_test=y_test['pca_rem'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8. Modelo de XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['rem']['symboling'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjunto de datos sin PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo aproximado: 7m\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train['rem'] = le.fit_transform(y_train['rem'])\n",
    "y_test['rem'] = le.transform(y_test['rem'])\n",
    "\n",
    "params = {'learning_rate': [0.03,0.3],\n",
    "                  'subsample'    : [1, 0.5, 0.2],\n",
    "                  'n_estimators' : [100,500,1000],\n",
    "                  'max_depth'    : [None,5,7]    \n",
    "            }\n",
    "\n",
    "tuning_model=GridSearchCV(xgb.XGBClassifier(random_state=1234),\n",
    "                          param_grid=params,scoring=kappa_scorer,cv=5,verbose=3)\n",
    "tuning_model.fit(X_train['rem'],y_train['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clsfr = xgb.XGBClassifier(random_state=1234,\n",
    "                               learning_rate=0.3,\n",
    "                               max_depth=None,\n",
    "                               n_estimators=500,\n",
    "                               subsample=1)\n",
    "\n",
    "model,metrics = classification_metrics(xgb_clsfr,X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])\n",
    "pickle.dump(model, open('models/XGBoostClassifierRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/XGBoostClassifierRemMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC_curve_xgb(model, X_train, y_train, X_test, y_test):\n",
    "    visualizer = ROCAUC(model, encoder={0: 'riesgo negativo',\n",
    "                                        1: 'neutral', \n",
    "                                        2: 'riesgo 1', \n",
    "                                        3: 'riesgo 2',\n",
    "                                        4: 'riesgo 3'})                                    \n",
    "    visualizer.fit(X_train, y_train)\n",
    "    visualizer.score(X_test, y_test)\n",
    "    visualizer.show()\n",
    "    \n",
    "    return visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_and_conf_matrix(xgb.XGBClassifier(random_state=1234,\n",
    "                               learning_rate=0.3,\n",
    "                               max_depth=None,\n",
    "                               n_estimators=500,\n",
    "                               subsample=1),\n",
    "                       X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve_xgb(xgb.XGBClassifier(random_state=1234,\n",
    "                               learning_rate=0.3,\n",
    "                               max_depth=None,\n",
    "                               n_estimators=500,\n",
    "                               subsample=1),\n",
    "                X_train=X_train['rem'], y_train=y_train['rem'],X_test=X_test['rem'], y_test=y_test['rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjunto de datos sin PCA y feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo aproximado: 7m\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train['rem'] = le.fit_transform(y_train['rem'])\n",
    "y_test['rem'] = le.transform(y_test['rem'])\n",
    "\n",
    "params = {'learning_rate': [0.03,0.3],\n",
    "                  'subsample'    : [1, 0.5, 0.2],\n",
    "                  'n_estimators' : [100,500,1000],\n",
    "                  'max_depth'    : [None,5,7]    \n",
    "            }\n",
    "\n",
    "X_t = SelectKBest(score_func=f_classif,k=31).fit_transform(X_train['rem'],y_train['rem'])\n",
    "\n",
    "tuning_model=GridSearchCV(xgb.XGBClassifier(random_state=1234),\n",
    "                          param_grid=params,scoring=kappa_scorer,cv=5,verbose=3)\n",
    "tuning_model.fit(X_t,y_train['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline_xgb = make_pipeline(SelectKBest(score_func=f_classif,k=31),\n",
    "                             xgb.XGBClassifier(random_state=1234,\n",
    "                               learning_rate=0.03,\n",
    "                               max_depth=None,\n",
    "                               n_estimators=500,\n",
    "                               subsample=1))\n",
    "\n",
    "model,metrics = classification_metrics(pipeline_xgb,X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])\n",
    "pickle.dump(model, open('models/XGBoostClassifierRemFS.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/XGBoostClassifierRemFSMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_xgb = make_pipeline(SelectKBest(score_func=f_classif,k=31),\n",
    "                             xgb.XGBClassifier(random_state=1234,\n",
    "                               learning_rate=0.03,\n",
    "                               max_depth=None,\n",
    "                               n_estimators=500,\n",
    "                               subsample=1))\n",
    "\n",
    "report_and_conf_matrix(pipeline_xgb,X_train['rem'],X_test['rem'],y_train['rem'],y_test['rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_xgb = make_pipeline(SelectKBest(score_func=f_classif,k=31),\n",
    "                             xgb.XGBClassifier(random_state=1234,\n",
    "                               learning_rate=0.03,\n",
    "                               max_depth=None,\n",
    "                               n_estimators=500,\n",
    "                               subsample=1))\n",
    "\n",
    "plot_ROC_curve_xgb(pipeline_xgb,X_train=X_train['rem'], y_train=y_train['rem'],X_test=X_test['rem'], y_test=y_test['rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjunto de datos con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo aproximado: 6m\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train['pca_rem'] = le.fit_transform(y_train['pca_rem'])\n",
    "y_test['pca_rem'] = le.transform(y_test['pca_rem'])\n",
    "\n",
    "params = {'learning_rate': [0.03,0.3],\n",
    "                  'subsample'    : [1, 0.5, 0.2],\n",
    "                  'n_estimators' : [100,500,1000],\n",
    "                  'max_depth'    : [None,5,7]    \n",
    "            }\n",
    "\n",
    "tuning_model=GridSearchCV(xgb.XGBClassifier(random_state=1234),\n",
    "                          param_grid=params,scoring=kappa_scorer,cv=5,verbose=3)\n",
    "tuning_model.fit(X_train['pca_rem'],y_train['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clsfr = xgb.XGBClassifier(random_state=1234,\n",
    "                               learning_rate=0.3,\n",
    "                               max_depth=7,\n",
    "                               n_estimators=1000,\n",
    "                               subsample=1)\n",
    "\n",
    "model,metrics = classification_metrics(xgb_clsfr,X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])\n",
    "pickle.dump(model, open('models/XGBoostClassifierPCARem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/XGBoostClassifierPCARemMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_and_conf_matrix(xgb.XGBClassifier(random_state=1234,\n",
    "                               learning_rate=0.3,\n",
    "                               max_depth=7,\n",
    "                               n_estimators=1000,\n",
    "                               subsample=1),\n",
    "                       X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve_xgb(xgb.XGBClassifier(random_state=1234,\n",
    "                               learning_rate=0.3,\n",
    "                               max_depth=7,\n",
    "                               n_estimators=1000,\n",
    "                               subsample=1),\n",
    "                X_train=X_train['pca_rem'], y_train=y_train['pca_rem'],X_test=X_test['pca_rem'], y_test=y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjunto de datos con PCA y feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo aproximado: 9m\n",
    "\n",
    "params = {'learning_rate': [0.03,0.3],\n",
    "                  'subsample'    : [1, 0.5, 0.2],\n",
    "                  'n_estimators' : [100,500,1000],\n",
    "                  'max_depth'    : [None,5,7]    \n",
    "            }\n",
    "\n",
    "X_t = SelectKBest(score_func=f_classif,k=27).fit_transform(X_train['pca_rem'],y_train['pca_rem'])\n",
    "\n",
    "tuning_model=GridSearchCV(xgb.XGBClassifier(random_state=1234),\n",
    "                          param_grid=params,scoring=kappa_scorer,cv=5,verbose=3)\n",
    "tuning_model.fit(X_t,y_train['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_xgb = make_pipeline(SelectKBest(score_func=f_classif,k=27),\n",
    "                             xgb.XGBClassifier(random_state=1234,\n",
    "                               learning_rate=0.03,\n",
    "                               max_depth=7,\n",
    "                               n_estimators=1000,\n",
    "                               subsample=1))\n",
    "\n",
    "model,metrics = classification_metrics(pipeline_xgb,X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])\n",
    "pickle.dump(model, open('models/XGBoostClassifierPCARemFS.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/XGBoostClassifierPCARemFSMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_xgb = make_pipeline(SelectKBest(score_func=f_classif,k=27),\n",
    "                             xgb.XGBClassifier(random_state=1234,\n",
    "                               learning_rate=0.03,\n",
    "                               max_depth=7,\n",
    "                               n_estimators=1000,\n",
    "                               subsample=1))\n",
    "\n",
    "report_and_conf_matrix(pipeline_xgb,X_train['pca_rem'],X_test['pca_rem'],y_train['pca_rem'],y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_xgb = make_pipeline(SelectKBest(score_func=f_classif,k=27),\n",
    "                             xgb.XGBClassifier(random_state=1234,\n",
    "                               learning_rate=0.03,\n",
    "                               max_depth=7,\n",
    "                               n_estimators=1000,\n",
    "                               subsample=1))\n",
    "\n",
    "plot_ROC_curve_xgb(pipeline_xgb,X_train=X_train['pca_rem'], y_train=y_train['pca_rem'],X_test=X_test['pca_rem'], y_test=y_test['pca_rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Comparación de los distintos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos las métricas de los modelos previamente guardados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = list()\n",
    "names = list()\n",
    "metrics.append(pickle.load(open('metrics/logisticRegressionRemMetrics.pkl','rb')))\n",
    "names.append('logisticRegressionRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/logisticRegressionRemFSMetrics.pkl','rb')))\n",
    "names.append('logisticRegressionRemFSMetrics')\n",
    "metrics.append(pickle.load(open('metrics/logisticRegressionPCARemMetrics.pkl','rb')))\n",
    "names.append('logisticRegressionPCARemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/logisticRegressionPCARemFSMetrics.pkl','rb')))\n",
    "names.append('logisticRegressionPCARemFSMetrics')\n",
    "metrics.append(pickle.load(open('metrics/KnnClassifierRemMetrics.pkl','rb')))\n",
    "names.append('KnnClassifierRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/KnnClassifierPCARemMetrics.pkl','rb')))\n",
    "names.append('KnnClassifiePCARemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/NBClassifierRemMetrics.pkl','rb')))\n",
    "names.append('NBClassifierRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/NBClassifierRemFSMetrics.pkl','rb')))\n",
    "names.append('NBClassifierRemFSMetrics')\n",
    "metrics.append(pickle.load(open('metrics/NBClassifierPCARemMetrics.pkl','rb')))\n",
    "names.append('NBClassifierPCARemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/NBClassifierPCARemFSMetrics.pkl','rb')))\n",
    "names.append('NBClassifierPCARemFSMetrics')\n",
    "metrics.append(pickle.load(open('metrics/DecisionTreeClassifierRemMetrics.pkl','rb')))\n",
    "names.append('DecisionTreeClassifierRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/DecisionTreeClassifierRemFSMetrics.pkl','rb')))\n",
    "names.append('DecisionTreeClassifierRemFSMetrics')\n",
    "metrics.append(pickle.load(open('metrics/DecisionTreeClassifierPCARemMetrics.pkl','rb')))\n",
    "names.append('DecisionTreeClassifierPCARemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/DecisionTreeClassifierPCARemFSMetrics.pkl','rb')))\n",
    "names.append('DecisionTreeClassifierPCARemFSMetrics')\n",
    "metrics.append(pickle.load(open('metrics/RandomForestClassifierRemMetrics.pkl','rb')))\n",
    "names.append('RandomForestClassifierRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/RandomForestClassifierRemFSMetrics.pkl','rb')))\n",
    "names.append('RandomForestClassifierRemFSMetrics')\n",
    "metrics.append(pickle.load(open('metrics/RandomForestClassifierPCARemMetrics.pkl','rb')))\n",
    "names.append('RandomForestClassifierPCARemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/RandomForestClassifierPCARemFSMetrics.pkl','rb')))\n",
    "names.append('RandomForestClassifierPCARemFSMetrics')\n",
    "metrics.append(pickle.load(open('metrics/GradientBoostingClassifierRemMetrics.pkl','rb')))\n",
    "names.append('GradientBoostingClassifierRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/GradientBoostingClassifierRemFSMetrics.pkl','rb')))\n",
    "names.append('GradientBoostingClassifierRemFSMetrics')\n",
    "metrics.append(pickle.load(open('metrics/GradientBoostingClassifierPCARemMetrics.pkl','rb')))\n",
    "names.append('GradientBoostingClassifierPCARemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/GradientBoostingClassifierPCARemFSMetrics.pkl','rb')))\n",
    "names.append('GradientBoostingClassifierPCARemFSMetrics')\n",
    "metrics.append(pickle.load(open('metrics/XGBoostClassifierRemMetrics.pkl','rb')))\n",
    "names.append('XGBoostClassifierRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/XGBoostClassifierRemFSMetrics.pkl','rb')))\n",
    "names.append('XGBoostClassifierRemFSMetrics')\n",
    "metrics.append(pickle.load(open('metrics/XGBoostClassifierPCARemMetrics.pkl','rb')))\n",
    "names.append('XGBoostClassifierPCARemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/XGBoostClassifierPCARemFSMetrics.pkl','rb')))\n",
    "names.append('XGBoostClassifierPCARemFSMetrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagrama de barras para observar el modelo más importante de acuerdo al Accuracy de prueba\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((20,20))\n",
    "ax.bar(names, [i['Test Accuracy'] for i in metrics])\n",
    "ax.set(title='Accuracy de prueba',xlabel='Modelos',ylabel='Score')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagrama de barras para observar el modelo más importante de acuerdo al Kappa de prueba\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((20,20))\n",
    "ax.bar(names, [i['Test Kappa'] for i in metrics])\n",
    "ax.set(title='Kappa de prueba',xlabel='Modelos',ylabel='Score')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se organizan los modelos de acuerdo a su Accuracy de prueba\n",
    "\n",
    "models = [(names[i],metrics[i]) for i in range(len(names))]\n",
    "models = sorted(models,key=lambda x:x[1]['Test Accuracy'],reverse=True)\n",
    "ranking = pd.DataFrame(columns=['Model','Test Accuracy'])\n",
    "for model in models:\n",
    "    ranking = pd.concat([ranking, pd.DataFrame([{'Model':model[0],'Test Accuracy':model[1]['Test Accuracy']}])],ignore_index=True)\n",
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se organizan los modelos de acuerdo a su Kappa de prueba\n",
    "\n",
    "models = [(names[i],metrics[i]) for i in range(len(names))]\n",
    "models = sorted(models,key=lambda x:x[1]['Test Kappa'],reverse=True)\n",
    "ranking = pd.DataFrame(columns=['Model','Test Kappa'])\n",
    "for model in models:\n",
    "    ranking = pd.concat([ranking, pd.DataFrame([{'Model':model[0],'Test Kappa':model[1]['Test Kappa']}])],ignore_index=True)\n",
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
