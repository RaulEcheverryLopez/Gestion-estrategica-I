{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto de Aprendizaje Supervisado\n",
    "- Con este proyecto aprenderemos como construir, entrenar y evaluar un modelo que resuelva una tarea de regresión.\n",
    "- Utilizaremos diversas librerías para la manipulación, análisis, visualización, modelado y evaluación de los datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Librerías básicas a utilizar\n",
    "\n",
    "- Pandas: para el análisis de datos a través de dataframes (data tabular)\n",
    "- Numpy: para todo lo relacionado con manipulación de arreglos y análisis numérico\n",
    "- Seaborn y matplotlib.pyplot: para visualización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "\n",
    "# Manejo de analísis de datos a través de dataframes (data tabular)\n",
    "import pandas as pd\n",
    "# Manipulación de arreglos y análisis numérico\n",
    "import numpy as np\n",
    "# Visualización de datos\n",
    "import seaborn as sns\n",
    "# Visualización de datos\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Librería para el manejo de expresiones regulares\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lectura de los datos\n",
    "\n",
    "- El primer paso es la Lectura del archivo csv a un dataframe de pandas\n",
    "- Se trata de una tarea de regresión ya que la variable objetivo es la esperanza de vida (tipo continua)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('Life_Expectancy_Data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al hacer el llamado al dataframe, podemos dar un primer vistazo al conjunto de datos. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análisis Exploratorio de Datos\n",
    "\n",
    "- Limpieza de datos\n",
    "- Exploración de datos\n",
    "- Ingeniería de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Limpieza de datos\n",
    "\n",
    "Para realizar una apropiada limpieza de datos se deben entender cada una de las variables presentes dentro del conjunto de datos.\n",
    "\n",
    "Elementos a tener en cuenta:\n",
    "- El significado y tipo (e.g. nominal/ordinal/intervalo/ratio) de cada una de las variables\n",
    "- Identificación de valores faltantes y en caso de haberlos plantear las opciones para tratarlos\n",
    "- Presencia de datos atípicos y en caso de haberlos identificar la manera de tratarlos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formateamos los nombres de las variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_columns(df):\n",
    "    result = df.copy()\n",
    "    new_cols = []\n",
    "    for col in result.columns:\n",
    "        new_cols.append(re.sub(r'\\s+', ' ',col.strip()).replace(' ','_').lower())\n",
    "    result.columns = new_cols\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripciones de variables\n",
    "\n",
    "- country (Nominal): país del que se están analizando los indicadores\n",
    "- year (Ordinal): año en el que fueron recopilados los indicadores (2000 a 2015)\n",
    "- status (Nominal): si se considera como una país desarrollado o en vía de desarrollo\n",
    "- life_expectancy (Ratio): la esperanza de vida de las personas de dicho país para ese año en particular \n",
    "- adult_mortality (Ratio): el promedio de mortalidad adulta de cada 1000 personas\n",
    "- infant_deaths (Ratio): el promedio de mortalidad infantil de cada 1000 personas\n",
    "- alcohol (Ratio): el promedio de consumo de alcohol medido en litros de alcohol puro per capita \n",
    "- percentage_expenditure (Ratio): gastos en salud como porcentaje del PIB \n",
    "- hepatitis_b (Ratio): número de niños de 1 año inmunizados contra la hepatitis B sobre toda la población de niños de 1 año de edad\n",
    "- measles (Ratio): número de casos reportados de sarampión 1000 personas \n",
    "- bmi (Interval/Ordinal): promedio de índice de masa corporal de la población total \n",
    "- under-five_deaths (Ratio): número de muertes de niños por debajo de los 5 años por 1000 personas\n",
    "- polio (Ratio): número de niños de 1 año inmunizados contra el polio sobre toda la población de niños de 1 año de edad\n",
    "- total_expenditure (Ratio): gasto gubernamental en salud como un porcentaje del total de gastos del gobierno \n",
    "- diphtheria (Ratio): Tasa de inmunización contra difteria, tétanos y tosferina (DTP3) en niños de 1 año\n",
    "- hiv/aids (Ratio): Muertes por cada 1000 nacidos vivos causadas por el VIH/SIDA en personas menores de 5 años \n",
    "- gdp (Ratio): PIB per capita \n",
    "- population (Ratio): población de un país\n",
    "- thinness_1-19_years (Ratio): tasa de delgadez entre personas de 10 a 19 años\n",
    "- thinness_5-9_years (Ratio): tasa de delgadez entre personas de 5 a 9 años\n",
    "- income_composition_of_resources (Ratio):  Índice de Desarrollo Humano en términos de composición del ingreso de los recursos (índice que varía de 0 a 1)\n",
    "- schooling (Ratio): número promedio de años de escolaridad de una población"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos renombrar una variable que tenía un nombre errado de acuerdo con su descripción, pues no era delgadez de 1 a 19 años sino de 10 a 19 años."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Creamos un Transformer para renombrar columnas\n",
    "class Rename_columns(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        result = X.copy()\n",
    "        result = format_columns(result)\n",
    "        result = result.rename(columns={'thinness_1-19_years':'thinness_10-19_years'})\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed = Rename_columns().fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos nuestro dataset con nuestras columnas renombradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores faltantes\n",
    "\n",
    "- Detección de valores faltantes\n",
    "- Manejo de valores faltantes\n",
    "\n",
    "### ¿Qué podemos hacer con estos?\n",
    "\n",
    "- Identificar valores faltantes (no necesariamente representados por null)\n",
    "- Imputarlos\n",
    "- Eliminarlos (registros o columnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para revisar datos faltantes podemos dibujar un mapa de calor\n",
    "# Nos permite encontrar la ubicación de dichos datos faltantes\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df_renamed.isnull(), yticklabels = False, cbar = False, cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los métodos info e isnull podemos encontrar rápidamente aquellos datos explícitamente nulos.\n",
    "\n",
    "Vamos a revisar aquellos que no son explícitamente nulos:\n",
    "- Utilizamos el método describe para revisar cada variable para revisar si su descripción tiene sentido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué no tiene sentido a partir de la descripción anterior?\n",
    "- Mortalidad adulta: Parece un error el tener tan solo una muerte para algún país.\n",
    "- Mortalidad infantil por cada 1000 personas: No parece factible tener 0 muertes o 1800  (dato atípico aunque posible si se tiene en cuenta un país con alta tasa de nacimientos y una población no tan alta).\n",
    "- Índice de masa corporal: 15 o menos se considera extremo bajo peso. 1 no tiene sentido. 40 o más es extremadamente obeso. 87.3 no parece real. \n",
    "- Muertes de niños de menos 5 años de edad: 0 no resulta real.\n",
    "- PIB: Un producto Interno Bruto de 1.68 dólares no parece posible.\n",
    "- Población: 34 personas como población para todo un país no es sensato. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos diagramas de cajas y bigotes para verificar las variables que parecieron no tener sentido\n",
    "plt.figure(figsize=(15,10))\n",
    "for i, col in enumerate(['adult_mortality', 'infant_deaths', 'bmi', 'under-five_deaths', 'gdp', 'population'], start=1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    df_renamed.boxplot(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque a partir de estos diagramas podemos observar datos atípicos, ciertamente algunos de ellos son errores y deberíamos cambiarlos a nulos.\n",
    "- Mortalidad adulta menor al quinto percentil\n",
    "- Mortalidad infantil de 0\n",
    "- Índice de Masa Corporal menor a 10 y mayor a 50\n",
    "- Muertes de niños menores a 5 años de 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Transformer personalizado que nos convierte a nulos los datos erróneos\n",
    "class Nullify_variables(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        result = X.copy()\n",
    "        mort_5_percentile = np.percentile(df.iloc[:,4].dropna(), 5)\n",
    "        result.iloc[:,4] = result.apply(lambda x: np.nan if x.adult_mortality < mort_5_percentile else x.adult_mortality, axis=1)\n",
    "        result.infant_deaths = result.infant_deaths.replace(0, np.nan)\n",
    "        result.bmi = result.apply(lambda x: np.nan if (x.bmi < 10 or x.bmi > 50) else x.bmi, axis=1)\n",
    "        result['under-five_deaths'] = result['under-five_deaths'].replace(0, np.nan)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nulls = Nullify_variables().fit_transform(df_renamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nulls.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos la transformación y asignamos los valores a un nuevo dataframe de tal manera que podamos analizar los resultados.\n",
    "\n",
    "A partir de los datos se observa que hay una cantidad considerable de valores nulos.\n",
    "\n",
    "La idea es analizar estos datos de manera que podamos tomar una decisión informado sobre como tratarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos da un análisis pormenorizado de las variables con valores nulos explícitos\n",
    "def nulls_breakdown(df):\n",
    "    df_cols = list(df.columns)\n",
    "    cols_total_count = len(list(df.columns))\n",
    "    cols_count = 0\n",
    "    for loc, col in enumerate(df_cols):\n",
    "        null_count = df[col].isnull().sum()\n",
    "        total_count = df[col].isnull().count()\n",
    "        percent_null = round(null_count/total_count*100, 2)\n",
    "        if null_count > 0:\n",
    "            cols_count += 1\n",
    "            print('[iloc = {}] {} has {} null values: {}% null'.format(loc, col, null_count, percent_null))\n",
    "    cols_percent_null = round(cols_count/cols_total_count*100, 2)\n",
    "    print('Out of {} total columns, {} contain null values; {}% columns contain null values.'.format(cols_total_count, cols_count, cols_percent_null))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_breakdown(df_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué obtenemos a partir de este análisis?\n",
    "- Casi la mitad de los valores de BMI son nulos. \n",
    "- Tiene sentido eliminar dicha columna.\n",
    "- Otras 15 columnas contienen valores nulos. \n",
    "- Para las restantes columnas se pueden imputar los datos teniendo en cuenta la media por año. (teniendo en cuenta que es una serie de tiempo)\n",
    "- Se puede usar KNN para imputar y luego comparar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Transformer personalizado que nos elimina valores nulos\n",
    "# Eliminamos los registros con valores faltantes en la variable objetivo\n",
    "# Eliminamos la columna BMI\n",
    "# Las otras columnas con valores faltantes las imputamos con la media por año\n",
    "class Remove_null_values_mean(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        result = X.copy()\n",
    "        result = result.drop(columns='bmi')\n",
    "        imputed_data = []\n",
    "        for year in list(result.year.unique()):\n",
    "            year_data = result[result.year == year].copy()\n",
    "            for col in list(year_data.columns)[3:]:\n",
    "                year_data[col] = year_data[col].fillna(year_data[col].dropna().mean()).copy()\n",
    "            imputed_data.append(year_data)\n",
    "        result = pd.concat(imputed_data).copy()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_nulls_mean = Remove_null_values_mean().fit_transform(df_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_breakdown(df_non_nulls_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_nulls_mean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para revisar datos faltantes podemos dibujar un mapa de calor\n",
    "# Nos permite encontrar la ubicación de dichos datos faltantes\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df_non_nulls_mean.isnull(), yticklabels = False, cbar = False, cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hay datos faltantes en este caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos una función que nos permite obtener el X y el y para el dataframe de Life expectancy\n",
    "\n",
    "def obtain_X_y(df):\n",
    "    return df.drop(columns=['life_expectancy']),df[['life_expectancy']]\n",
    "\n",
    "# Segunda aproximación: utilizar knn para imputar los datos faltantes\n",
    "# Utilizamos KNNImputer para completar los datos faltantes\n",
    "# Para optimizar debemos encontrar el mejor k\n",
    "\n",
    "from sklearn.impute import KNNImputer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error \n",
    "\n",
    "def optimize_k(df):\n",
    "    result = df.copy()\n",
    "    errors = []\n",
    "    result = result.drop(columns='bmi')\n",
    "    numeric_vars = result.select_dtypes('number').columns\n",
    "    for k in range(1, 20, 2):\n",
    "        imputer = KNNImputer(n_neighbors=k)\n",
    "        imputed = imputer.fit_transform(result[numeric_vars])\n",
    "        df_imputed = pd.DataFrame(imputed, columns=numeric_vars)\n",
    "        \n",
    "        X,y = obtain_X_y(df_imputed)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)\n",
    "\n",
    "        model = RandomForestRegressor(random_state=1234)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        error = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        errors.append({'K': k, 'RMSE': error, 'R^2': r2_score(y_test,preds)})\n",
    "        \n",
    "    return errors\n",
    "\n",
    "errors = optimize_k(df_nulls)\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica para encontrar el mejor k teniendo en cuenta el RMSE\n",
    " \n",
    "ax = sns.lineplot(x=[errors[i]['K'] for i in range(len(errors))],y=[errors[i]['RMSE'] for i in range(len(errors))])\n",
    "ax.set(title='RMSE vs K neighbors',xlim=(1,21),xlabel='K neighbors', ylabel='RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica para encontrar el mejor k teniendo en cuenta el R^2\n",
    "\n",
    "ax = sns.lineplot(x=[errors[i]['K'] for i in range(len(errors))],y=[errors[i]['R^2'] for i in range(len(errors))])\n",
    "ax.set(title='R^2 vs K neighbors',xlim=(1,21),xlabel='K neighbors', ylabel='R^2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenamos los resultados con base al mejor R^2 y el mejor RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos el mejor K teniendo en cuenta el R^2 y el RMSE\n",
    "res1 = sorted(errors,key=lambda x: x['R^2'],reverse=False)[0]\n",
    "res2 = sorted(errors,key=lambda x: x['R^2'],reverse=True)[0]\n",
    "print('K =',res1['K'],' R^2 = ',res1['R^2'],' RMSE = ',res1['RMSE'])\n",
    "print('K =',res2['K'],' RMSE = ',res2['RMSE'],' R^2 = ',res2['R^2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos los mejores valores son k = 1 y k = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luego de reconocer a k=19 como el mejor número de vecinos más cercanos, \n",
    "# podemos imputar nuestros datos faltantes\n",
    "\n",
    "# Creamos un Transformer personalizado para imputar con KNN\n",
    "class KnnImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        result = X.copy()\n",
    "        result = result.drop(columns='bmi')\n",
    "        numeric_vars = result.select_dtypes('number').columns\n",
    "        imputed = KNNImputer(n_neighbors=self.k).fit_transform(result[numeric_vars])\n",
    "        df_imputed = pd.DataFrame(imputed, columns=numeric_vars)\n",
    "        for col in X.select_dtypes('object'):\n",
    "            df_imputed[col] = X[col]\n",
    "        return df_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed_knn = KnnImputer(19).fit_transform(df_nulls)\n",
    "df_imputed_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores atípicos\n",
    "Visualizamos utilizando\n",
    "- Diagramas de cajas y bigotes\n",
    "- Histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para graficar los valores continuos del dataframe\n",
    "def outliers_visual(data):\n",
    "    cont_vars = list(data.select_dtypes('number').columns)\n",
    "    plt.figure(figsize=(15, 40))\n",
    "    i = 0\n",
    "    val = int(len(cont_vars)/2) + 1\n",
    "    for col in cont_vars:\n",
    "        i += 1\n",
    "        plt.subplot(val, 4, i)\n",
    "        plt.boxplot(data[col])\n",
    "        plt.title('{} boxplot'.format(col))\n",
    "        i += 1\n",
    "        plt.subplot(val, 4, i)\n",
    "        plt.hist(data[col])\n",
    "        plt.title('{} histogram'.format(col))\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_visual(df_non_nulls_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando los gráficos anteriores podemos observar las distribuciones para cada variable continua.\n",
    "\n",
    "Visualmente resulta claro que existen múltiples outliers, incluyendo la variable objetivo (esperanza de vida)\n",
    "\n",
    "Podemos corroborar lo anterior de manera estadística haciendo uso del método de Tukey (se consideran como datos atípicos aquellos que están 1.5 veces por fuera del rango intercuartil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular los datos atípicos utilizando el método de Tukey\n",
    "def outlier_count(col, data):\n",
    "    print(15*'-' + col + 15*'-')\n",
    "    q75, q25 = np.percentile(data[col], [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    min_val = q25 - (iqr*1.5)\n",
    "    max_val = q75 + (iqr*1.5)\n",
    "    outlier_count = len(np.where((data[col] > max_val) | (data[col] < min_val))[0])\n",
    "    outlier_percent = round(outlier_count/len(data[col])*100, 2)\n",
    "    print('Number of outliers: {}'.format(outlier_count))\n",
    "    print('Percent of data that is outlier: {}%'.format(outlier_percent))\n",
    "    return outlier_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar las columnas de tipo continuas con datos atípicos\n",
    "cont_vars = []\n",
    "for col in list(df_non_nulls_mean.select_dtypes('number').columns):\n",
    "    if outlier_count(col, df_non_nulls_mean) > 0:\n",
    "        cont_vars.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué hacer con tantos datos atípicos?\n",
    "Existen varias opciones a considerar\n",
    "- Eliminarlos (mejor evitar esta opción para mantener la mayor cantidad de información posible)\n",
    "- Limitar los límites inferiores y superiores (winzorize)\n",
    "- Transformación de los datos (normalización)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo en cuenta los gráficos previos y los estadísticos, se puede ver que existen cantidades diferentes de outliers para cada variable y hacia diferentes direcciones. Es por esto que en este caso la mejor decisión es limitar a través de winsorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "wins_dict = {}\n",
    "\n",
    "def test_wins(col, df, wins_dict, lower_limit=0, upper_limit=0, show_plot=True):\n",
    "    wins_data = winsorize(df[col], limits=(lower_limit, upper_limit))\n",
    "    wins_dict[col] = wins_data\n",
    "    if show_plot == True:\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.subplot(121)\n",
    "        plt.boxplot(df[col])\n",
    "        plt.title('original {}'.format(col))\n",
    "        plt.subplot(122)\n",
    "        plt.boxplot(wins_data)\n",
    "        plt.title('wins=({},{}) {}'.format(lower_limit, upper_limit, col))\n",
    "        plt.show()\n",
    "    return wins_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificación de la winsorizing\n",
    "wins_dict = {}\n",
    "wins_dict = test_wins(cont_vars[0], df_non_nulls_mean, wins_dict, lower_limit=.01, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[1], df_non_nulls_mean, wins_dict, upper_limit=.04, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[2], df_non_nulls_mean, wins_dict, upper_limit=.05, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[3], df_non_nulls_mean, wins_dict, upper_limit=.0025, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[4], df_non_nulls_mean, wins_dict, upper_limit=.135, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[5], df_non_nulls_mean, wins_dict, lower_limit=.1, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[6], df_non_nulls_mean, wins_dict, upper_limit=.19, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[7], df_non_nulls_mean, wins_dict, upper_limit=.05, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[8], df_non_nulls_mean, wins_dict, lower_limit=.1, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[9], df_non_nulls_mean, wins_dict, upper_limit=.02, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[10], df_non_nulls_mean, wins_dict, lower_limit=.105, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[11], df_non_nulls_mean, wins_dict, upper_limit=.185, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[12], df_non_nulls_mean, wins_dict, upper_limit=.105, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[13], df_non_nulls_mean, wins_dict, upper_limit=.07, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[14], df_non_nulls_mean, wins_dict, upper_limit=.035, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[15], df_non_nulls_mean, wins_dict, upper_limit=.035, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[16], df_non_nulls_mean, wins_dict, lower_limit=.05, show_plot=True)\n",
    "wins_dict = test_wins(cont_vars[17], df_non_nulls_mean, wins_dict, lower_limit=.025, upper_limit=.005, show_plot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A través de los diagramas de cajas y bigotes se puede detallar como se logra mitigar el impacto de los outliers para cada una de las variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Transformer personalizado para limitar los outliers con el winsorizing\n",
    "class Winsorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, wins, cont_vars):\n",
    "        self.wins = wins\n",
    "        self.cont_vars = cont_vars\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        wins_df =pd.DataFrame(index = X.index)\n",
    "        for col in ['year','status','life_expectancy','country']:\n",
    "            wins_df[col] = X[col]\n",
    "        for col in self.cont_vars:\n",
    "                wins_df[col] = self.wins[col]     \n",
    "        return wins_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wins_mean = Winsorizer(wins_dict,cont_vars).fit_transform(df_non_nulls_mean)\n",
    "df_wins_mean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_visual(df_imputed_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar las columnas de tipo continuas con datos atípicos\n",
    "cont_vars1 = []\n",
    "for col in list(df_imputed_knn.select_dtypes('number').columns):\n",
    "    if outlier_count(col, df_imputed_knn) > 0:\n",
    "        cont_vars1.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_vars1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificación de la winsorizing\n",
    "wins_dict1 = {}\n",
    "wins_dict1 = test_wins(cont_vars1[0], df_imputed_knn, wins_dict1, lower_limit=.01, show_plot=True)\n",
    "wins_dict1 = test_wins(cont_vars1[1], df_imputed_knn, wins_dict1, upper_limit=.04, show_plot=True)\n",
    "wins_dict1 = test_wins(cont_vars1[2], df_imputed_knn, wins_dict1, upper_limit=.12, show_plot=True)\n",
    "wins_dict1 = test_wins(cont_vars1[3], df_imputed_knn, wins_dict1, upper_limit=.0025, show_plot=True)\n",
    "wins_dict1 = test_wins(cont_vars1[4], df_imputed_knn, wins_dict1, upper_limit=.133, show_plot=True)\n",
    "wins_dict1 = test_wins(cont_vars1[5], df_imputed_knn, wins_dict1, lower_limit=.1, show_plot=True)\n",
    "wins_dict1 = test_wins(cont_vars1[6], df_imputed_knn, wins_dict1, upper_limit=.19, show_plot=True)\n",
    "wins_dict1 = test_wins(cont_vars1[7], df_imputed_knn, wins_dict1, upper_limit=.15, show_plot=True)\n",
    "wins_dict1 = test_wins(cont_vars1[8], df_imputed_knn, wins_dict1, lower_limit=.11, show_plot=True)\n",
    "wins_dict1 = test_wins(cont_vars1[9], df_imputed_knn, wins_dict1, upper_limit=.02, show_plot=True)\n",
    "wins_dict1 = test_wins(cont_vars1[10], df_imputed_knn, wins_dict1, lower_limit=.105, show_plot=True)\n",
    "wins_dict1 = test_wins(cont_vars1[11], df_imputed_knn, wins_dict1, upper_limit=.185, show_plot=True)\n",
    "wins_dict1 = test_wins(cont_vars1[12], df_imputed_knn, wins_dict1, upper_limit=.17, show_plot=True)\n",
    "wins_dict1 = test_wins(cont_vars1[13], df_imputed_knn, wins_dict1, upper_limit=.12, show_plot=True)\n",
    "wins_dict1 = test_wins(cont_vars1[14], df_imputed_knn, wins_dict1, upper_limit=.035, show_plot=True)\n",
    "wins_dict1 = test_wins(cont_vars1[15], df_imputed_knn, wins_dict1, upper_limit=.035, show_plot=True)\n",
    "wins_dict1 = test_wins(cont_vars1[16], df_imputed_knn, wins_dict1, lower_limit=.05, show_plot=True)\n",
    "wins_dict1 = test_wins(cont_vars1[17], df_imputed_knn, wins_dict1, lower_limit=.025, upper_limit=.005, show_plot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wins_knn = Winsorizer(wins_dict1,cont_vars1).fit_transform(df_imputed_knn)\n",
    "df_wins_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Exploración de datos\n",
    "- Análisis univariado\n",
    "    - Variables continuas\n",
    "    - Variables categóricas\n",
    "- Análisis Bivariado\n",
    "    - Variables continuas comparadas con variable objetivo y entre sí\n",
    "    - Variables categóricas con respecto a variable objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis univariado\n",
    "\n",
    "La idea con este este análisis es ver cada variable por sí sola. Generalmente se realiza con la ayuda de histogramas para variables continuas y countplots o barplot para datos categóricos. Además de incluir los reportes estadísticos que no deben faltar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a revisar primero el dataframe con los datos imputados con la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptores estadísticos para valores continuos\n",
    "df_wins_mean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptores estadísticos para valores categóricos\n",
    "df_wins_mean.describe(include='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para visualizar las distribuciones de las variables continuas\n",
    "def visualize_distributions(df):\n",
    "    cols = df.select_dtypes('number').columns\n",
    "    val = int(len(cols)/2)+1\n",
    "    plt.figure(figsize=(15, 40))\n",
    "    for i, col in enumerate(cols, 1):\n",
    "        plt.subplot(val, 4, i)\n",
    "        plt.hist(df[col])\n",
    "        plt.title(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos la distribución de las variables continuas\n",
    "visualize_distributions(df_wins_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de los descriptores estadísticos y la visualización de las distribuciones se nota que el winsorizing tuvo un efecto considerable en algunas variables (distribuciones no tan sesgadas, como por ejemplo income_composition_of_resources, schooling), aunque no tanto en otras (e.g. population, gdp). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para visualizar registros por país\n",
    "def visualize_rows_by_country(df):\n",
    "    plt.figure(figsize=(15, 25))\n",
    "    df.country.value_counts(ascending=True).plot(kind='barh')\n",
    "    plt.title('Count of Rows by Country')\n",
    "    plt.xlabel('Count of Rows')\n",
    "    plt.ylabel('Country')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_rows_by_country(df_wins_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque no resulta una gráfica muy amigable, se puede observar que la gran mayoría de países tienen 16 registros (16 años). Podemos notar que los países no están sobrerepresentados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para visualizar registros por año\n",
    "def visualize_rows_by_year(df):\n",
    "    df.year.value_counts().sort_index().plot(kind='barh')\n",
    "    plt.title('Count of Rows by Year')\n",
    "    plt.xlabel('Count of Rows')\n",
    "    plt.ylabel('Year')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_rows_by_year(df_wins_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que todos los años tienen la misma cantidad de registros, con excepción del 2013. Tal vez en dicho año aparecieron los registros de aquellos países que tan solamente tuvieron 1 registro.\n",
    "\n",
    "No pareciese significativo para los efectos resultantes de este análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_country_status(df):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(121)\n",
    "    df.status.value_counts().plot(kind='bar')\n",
    "    plt.title('Count of Rows by Country Status')\n",
    "    plt.xlabel('Country Status')\n",
    "    plt.ylabel('Count of Rows')\n",
    "    plt.xticks(rotation=0)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    df.status.value_counts().plot(kind='pie', autopct='%.2f')\n",
    "    plt.ylabel('')\n",
    "    plt.title('Country Status Pie Chart')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_country_status(df_wins_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A través de estos dos gráficos podemos observar que la gran mayoría de los datos provienen de países en vía de desarrollo. Claramente cualquier modelo que podamos llegar a implementar haciendo uso de estos datos arrojará datos más exactos para aquellos países en desarrollo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptores estadísticos para valores continuos\n",
    "df_wins_knn.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptores estadísticos para valores categóricos\n",
    "df_wins_knn.describe(include='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos la distribución de las variables continuas\n",
    "visualize_distributions(df_wins_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_rows_by_country(df_wins_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_rows_by_year(df_wins_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_country_status(df_wins_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis Bivariado\n",
    "Aspectos de relevancia a tener en cuenta:\n",
    "- Variables continuas con respecto a esperanza de vida (independientes vs dependiente) y entre ellas\n",
    "- Variables categóricas con respecto a esperanza de vida (independientes vs dependiente)\n",
    "- Variables categóricas con respecto a continuas (Caso de año y país vs las demás)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación entre variables continuas\n",
    "df_wins_mean[df_wins_mean.select_dtypes('number').columns].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df,y=True):\n",
    "    if y:\n",
    "        cont_vars = list(df.drop(['year'],axis=1).select_dtypes('number').columns)\n",
    "    else:\n",
    "        cont_vars = list(df.select_dtypes('number').columns)\n",
    "    mask = np.triu(df[cont_vars].corr())\n",
    "    plt.figure(figsize=(15,15))\n",
    "    sns.heatmap(df[cont_vars].corr(), annot=True, fmt='.2g', vmin=-1, vmax=1, center=0, cmap='coolwarm', mask=mask)\n",
    "    plt.ylim(18, 0)\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix(df_wins_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando la matriz podemos observar importantes correlaciones entre variables\n",
    "- Esperanza de vida está altamente correlacionada con:\n",
    "    - Mortalidad adulta (negativa)\n",
    "    - HIV/AIDS (negativa)\n",
    "    - income_composition_of_resources (positiva)\n",
    "    - schooling (positiva)\n",
    "- La variable objetivo se encuentra bajamente correlacionada con la población\n",
    "- infant_deaths y under_five_deaths evidentemente están altamente correlacionadas\n",
    "- perecentage_expenditure se encuentra altamente correlacionada con gdp\n",
    "- hepatitis_b está altamente correlacionada con polio y diphteria\n",
    "- polio y diphteria están altamente correlacionadas\n",
    "- HIV/AIDS se encuentra negativamente correlacionada con with income_composition_of_resources\n",
    "- thinness_5-9_years está altamente correlacionada con thinness_10-15_years\n",
    "- income_composition_of_resources está altamente relacionada con schooling\n",
    "\n",
    "Se debe evitar variables independientes altamente correlacionadas entre sí. Igualmente, las variables independientes altamente correlacionadas con la variable dependiente pueden resultar más importante a la hora de implementar los modelos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_year_life_expectancy(df):\n",
    "    sns.lineplot(data=df, x='year', y='life_expectancy', marker='o')\n",
    "    plt.title('Life Expectancy by Year')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_year_life_expectancy(df_wins_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir del gráfico anterior podemos observar una tendencia, hay que corroborar si resulta significativa para incluirla dentro del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wins_mean.year.corr(df_wins_mean.life_expectancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe una correlación débil, se debe indagar a más profundidad si las diferencias entre años resultan considerables para considerarlos distintos.\n",
    "\n",
    "Un A-test resultaría útil para verificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def t_testing_years(df):\n",
    "    years = list(df.year.unique())\n",
    "    years.sort()\n",
    "    yearly_le = {}\n",
    "    for year in years:\n",
    "        year_data = df[df.year == year].life_expectancy\n",
    "        yearly_le[year] = year_data\n",
    "    for year in years[:-1]:\n",
    "        print(10*'-' + str(year) + ' to ' + str(year+1) + 10*'-')\n",
    "        print(stats.ttest_ind(yearly_le[year], yearly_le[year+1], equal_var=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_testing_years(df_wins_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo en cuenta que para un t-test si el valor del p-value no es menor que 0.05 no es considerado como estadísticamente significativo, podemos observar que la diferencia entre años para la esperanza de vida no resulta significativa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora revisamos para el caso de status. Observando la diferencia entre develping y developed con respecto de la esperanza de vida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wins_mean.groupby('status').life_expectancy.agg(['mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que los países desarrollados parecen tener una esperanza de vida más alta. Para corroborar podemos utilizar un t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_testing_status(df):\n",
    "    developed_le = df[df.status == 'Developed'].life_expectancy\n",
    "    developing_le = df[df.status == 'Developing'].life_expectancy\n",
    "    print(stats.ttest_ind(developed_le, developing_le, equal_var=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_testing_status(df_wins_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir del p-value se puede corroborar que las diferencias son muy significativas. Lo cual nos indica que status puede resultar importante para la implementación de futuros modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Ingeniería de características\n",
    "\n",
    "Vamos a eliminar y/o crear nuevas características que nos resulten útiles para la futura implementación de nuestros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Transformer personalizado para dummificar la variable status y eliminar\n",
    "# las variables year y country\n",
    "class Dummify(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        result = X.copy()\n",
    "        result = result.drop(columns=['year','country'])\n",
    "        result = pd.get_dummies(result,columns=['status'],dtype=float)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wins_mean_dummified = Dummify().fit_transform(df_wins_mean)\n",
    "df_wins_mean_dummified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wins_knn_dummified = Dummify().fit_transform(df_wins_knn)\n",
    "df_wins_knn_dummified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix(df_wins_mean_dummified,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes variables podemos considerarlas como altamente correlacionadas entre sí (correlación > .7 o correlación < -.7):\n",
    "\n",
    "- infant_deaths/under_five_deaths: 0.97 (hacer drop de infant_deaths ya que under_five_deaths está más altamente correlacionados con la esperanza de vida)\n",
    "- gdp/percentage_expenditure: 0.71 (hacer drop de percentage_expenditure ya que gdp está más altamente correlacionados con la esperanza de vida)\n",
    "- polio/diphtheria: 0.86 (hacer drop de polio ya que diphteria está más altamente correlacionados con la esperanza de vida)\n",
    "- thinness_5-9_years/thinness_10-19_years: 0.94 (hacer drop de thinness_10-19_years ya que thinness_5-9_years está ligeramente más altamente correlacionados con la esperanza de vida)\n",
    "- income_composition_of_resources/schooling (hacer drop de schooling ya que income_composition_of_resource está más altamente correlacionados con la esperanza de vidad)\n",
    "- status_Developed/status_Developing (hacer drop de status_Developing ya que uno es el opuesto del otro)\n",
    "\n",
    "Puede resultar útil eliminar las variables que no se encuentren muy correlacionadas con la esperanza de vida.\n",
    "- population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Transformer personalizado para realizar la eliminación de variables\n",
    "# a partir de sus correlaciones\n",
    "class Remove_highly_correlated_features(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        result = X.copy()\n",
    "        result = result.drop(columns=['infant_deaths', 'percentage_expenditure','polio',\n",
    "                                      'thinness_10-19_years','schooling','status_Developing',\n",
    "                                      'population'])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wins_mean_dummified_removed = Remove_highly_correlated_features().fit_transform(df_wins_mean_dummified)\n",
    "df_wins_knn_dummified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix(df_wins_mean_dummified_removed,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos uso de un pipeline para dejar listos nuestros dataframes que posteriormente utilizaremos para entrenar nuestros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe_mean_rem = Pipeline(steps = [('rename columns',Rename_columns()),\n",
    "                   ('nullify values',Nullify_variables()),\n",
    "                   ('remove null values mean',Remove_null_values_mean()),\n",
    "                   ('winsorize values',Winsorizer(wins_dict,cont_vars)),\n",
    "                   ('Dummify categorical variables',Dummify()),\n",
    "                   ('Remove highly correlated features',Remove_highly_correlated_features())])\n",
    "\n",
    "pipe_knn_rem = Pipeline(steps = [('rename columns',Rename_columns()),\n",
    "                   ('nullify values',Nullify_variables()),\n",
    "                   ('remove null values mean',KnnImputer(19)),\n",
    "                   ('winsorize values',Winsorizer(wins_dict,cont_vars)),\n",
    "                   ('Dummify categorical variables',Dummify()),\n",
    "                   ('Remove highly correlated features',Remove_highly_correlated_features())])\n",
    "\n",
    "\n",
    "df_pipeline_mean_rem = pipe_mean_rem.fit_transform(df)\n",
    "df_pipeline_knn_rem = pipe_knn_rem.fit_transform(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Protocolos de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con nuestra primera aproximación (imputación con la media)\n",
    "# Eliminación de variables altamente correlacionadas\n",
    "# Utilizamos el holdout con un 30% para la prueba y 70% para el entrenamiento\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dict()\n",
    "y = dict()\n",
    "X_train = dict()\n",
    "y_train = dict()\n",
    "X_test = dict()\n",
    "y_test = dict()\n",
    "\n",
    "\n",
    "X['mean_rem'], y['mean_rem'] = obtain_X_y(df_pipeline_mean_rem)\n",
    "\n",
    "X_train['mean_rem'], X_test['mean_rem'], y_train['mean_rem'], y_test['mean_rem'] = train_test_split(X['mean_rem'],y['mean_rem'],random_state=1234,test_size=0.3)\n",
    "print(X_train['mean_rem'].shape)\n",
    "print(X_test['mean_rem'].shape)\n",
    "print(y_train['mean_rem'].shape)\n",
    "print(y_test['mean_rem'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con nuestra segunda aproximación (imputación con knn con k=19)\n",
    "# Eliminando variables altamente correlacionadas\n",
    "\n",
    "X['knn_rem'], y['knn_rem'] = obtain_X_y(df_pipeline_knn_rem)\n",
    "\n",
    "X_train['knn_rem'], X_test['knn_rem'], y_train['knn_rem'], y_test['knn_rem'] = train_test_split(X['knn_rem'],y['knn_rem'],random_state=1234,test_size=0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Métricas de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementamos una función que nos permita calcular las métricas de regresión para el conjunto \n",
    "# de entrenamiento y de prueba\n",
    "# R2, R2 Ajustado MAE, MAPE, MSE, RMSE\n",
    "# El coeficiente de determinación se puede ver como la varianza total explicada por el modelo, \n",
    "# es decir la proporción de la varianza en la variable dependiente que se puede predecir a partir de \n",
    "# las variables independientes\n",
    "# Nos retorna el modelo entrenado y un diccionario con las principales métricas sobre el conjunto de\n",
    "# entrenamiento y el de prueba\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "\n",
    "def regression_metrics(model,X_train,X_test,y_train,y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    k_train = X_train.shape[1]\n",
    "    n_train = len(X_train)\n",
    "    k_test = X_test.shape[1]\n",
    "    n_test = len(X_test)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    adj_r2_train = 1-(1-r2_train)*(n_train-1)/(n_train-k_train-1)\n",
    "    adj_r2_test = 1-(1-r2_test)*(n_test-1)/(n_test-k_test-1)\n",
    "    metrics = {\"Training R^2\": r2_train,\n",
    "               \"Test R^2\": r2_test,\n",
    "               \"Training Adj R^2\": adj_r2_train,\n",
    "               \"Test Adj R^2\": adj_r2_test,\n",
    "               \"Training MAE\": mean_absolute_error(y_train,y_pred_train),\n",
    "               \"Test MAE\": mean_absolute_error(y_test,y_pred_test),\n",
    "               \"Training MAPE\": mean_absolute_percentage_error(y_train,y_pred_train),\n",
    "               \"Test MAPE\": mean_absolute_percentage_error(y_test,y_pred_test),\n",
    "               \"Training RMSE\": np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "               \"Test RMSE\": np.sqrt(mean_squared_error(y_test, y_pred_test))    \n",
    "               }\n",
    "    for item in metrics.items():\n",
    "        print(item[0],\"=\",item[1])\n",
    "    return model,metrics\n",
    "    \n",
    "def r2_metrics(model,X_train,X_test,y_train,y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    k_train = X_train.shape[1]\n",
    "    n_train = len(X_train)\n",
    "    k_test = X_test.shape[1]\n",
    "    n_test = len(X_test)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    adj_r2_train = 1-(1-r2_train)*(n_train-1)/(n_train-k_train-1)\n",
    "    adj_r2_test = 1-(1-r2_test)*(n_test-1)/(n_test-k_test-1)\n",
    "    return r2_train, r2_test, adj_r2_train, adj_r2_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementación de modelos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Baseline\n",
    "Como punto de partida para poder comparar cualquier modelo de regresión que pensemos implementar, debemos establecer un baseline. Al estar tratando con una tarea de regresión, hacemos uso de un Dummy regressor con estrategía de la media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecemos el baseline a partir de un dummy regressor\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "print('Using a dataframe imputed with the mean and with highly correlated variables removed')\n",
    "regression_metrics(DummyRegressor(strategy='mean'),X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])\n",
    "print()\n",
    "print('Using a dataframe imputed with knn and with highly correlated variables removed')\n",
    "regression_metrics(DummyRegressor(strategy='mean'),X_train['knn_rem'],X_test['knn_rem'],y_train['knn_rem'],y_test['knn_rem'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por supuesto podemos observar que los resultados no son buenos con este regresor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Modelo de K vecinos más cercanos\n",
    "Un algoritmo simple pero bastante útil que nos sirve para resolver tanto tareas de clasificación como de regresión. \n",
    "\n",
    "- Es fácil de entender e implementar\n",
    "- No hace suposiciones sobre la distribución de los datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procederemos utilizando nuestros dataframes resultado de las etapas previas del proceso de analítica de datos.\n",
    "\n",
    "Iniciamos con el dataframe imputado con la media y en el que eliminamos aquellas variables que se encontraban altamente correlacionadas entre sí (para evitar problemas de multicolinealidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos knn\n",
    "# Con el dataset imputado con la media y eliminando las variables \n",
    "# altamente correlacionadas\n",
    "# Utilizamos el Knn regressor con sus parámetros por defecto\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knnReg = KNeighborsRegressor()\n",
    "regression_metrics(knnReg,X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que knn no nos brinda un tan buen resultado. Evidentemente no es porque este sea un mal modelo, el problema es que este modelo se basa en distancias y si las escalas de las variables son diferentes, las decisiones del modelo estarán inapropiadamente dominadas por aquellas con mayor escala.\n",
    "\n",
    "Por ello podemos normalizar o estandarizar los datos y verificar los resultados nuevamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se debe tener en cuenta las diferencias en los procesos de escalamiento:\n",
    "- Normalizar: las observaciones se cambian y reescalan de manera que terminen en un rango entre 0 y 1. También se le conoce como escalamiento min-max X_new = (X_ — X_min)/(X_max — X_min). No maneja muy bien los datos atípicos\n",
    "- Estandarizar: es el proceso de poner distintas variables en la misma escala. Para cada observación de la variable se le substrae la media y se divide por la desviación estándar. La estandarización asume que los datos siguen una distribución normal. En este caso el resultado tienen sentido pues estamos utilizando un modelo no paramétrico, no lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "print('Stadardization')\n",
    "knnReg_st = make_pipeline(StandardScaler(),KNeighborsRegressor())\n",
    "regression_metrics(knnReg_st,X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])\n",
    "print()\n",
    "print('Normalization')\n",
    "knnReg_norm = make_pipeline(MinMaxScaler(),KNeighborsRegressor())\n",
    "regression_metrics(knnReg_norm,X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizando los resultados podemos observar que en ambos casos, al utilizar procesos de escalamiento, se mejoran los resultados.\n",
    "\n",
    "Ahora buscamos el mejor valor de k (único hiperparámetro de este modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos encontrar el mejor k para nuestro modelo\n",
    "# Utilizamos un pipeline para incluir la estandarización y el regresor knn \n",
    "\n",
    "for k in range(1,21,2):\n",
    "    print('Results for k =',k)\n",
    "    knnReg_st = make_pipeline(StandardScaler(),KNeighborsRegressor(n_neighbors=k))\n",
    "    regression_metrics(knnReg_st,X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos el R^2\n",
    "\n",
    "results = [r2_metrics(make_pipeline(StandardScaler(),KNeighborsRegressor(n_neighbors=i)),\n",
    "            X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])[1] for i in range(1,21,2)]\n",
    "ax = sns.lineplot(x=range(1,21,2),\n",
    "             y=results)\n",
    "ax.set(title=\"R^2 vs K neighbors\",ylabel='R^2',xlabel='K neighbors')\n",
    "kvals = [(col,i) for col,i in zip(results,range(1,21,2))]\n",
    "\n",
    "print(sorted(kvals, key=lambda x: x[0],reverse=True)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de K vecinos más cercanos con k=9, datos imputados con la media y eliminando variables altamente correlacionadas.\n",
    "\n",
    "Hacemos uso de pickle para guardar tanto el modelo ya entrenado como el resultado de las métricas de manera que al finalizar podamos comparar entre todos los modelos implementados y elegir el que presente mejores resultados en cuanto a las métricas más importantes sobre tareas de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Guardamos el mejor modelo utilizando k=9 con todas las variables independientes e imputación con la media\n",
    "\n",
    "model,metrics = regression_metrics(make_pipeline(StandardScaler(),KNeighborsRegressor(n_neighbors=9)),\n",
    "                                   X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])\n",
    "pickle.dump(model, open('models/KnnRegressorImpMeanRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/KnnRegressorImpMeanRemMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de características\n",
    "\n",
    "Aunque previamente, en la etapa de ingeniería de características habíamos creado nuevas variables y eliminado otras, en esta etapa verificaremos la importancia de las variables independientes con respecto a la objetivo utilizando distintos métodos. \n",
    "\n",
    "Esto con el fin de hallar el mejor grupo de variables que sirvan como insumo para nuestros modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos un método para extracción recursiva de características utilizando validación cruzada\n",
    "# Con este método obtenemos las variables independientes más importantes dentro del dataframe \n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "estimator = RandomForestRegressor(n_estimators=100)\n",
    "selector = RFECV(estimator, step=1, cv=5)\n",
    "selector = selector.fit(X_train['mean_rem'], y_train['mean_rem'])\n",
    " \n",
    "print(\"Ranking de características\", selector.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos el método SelectKBest que nos permite seleccionar las  K mejores características utilizando como función de puntaje el f-regression.\n",
    "\n",
    "El f-regression realiza una prueba estadística F entre cada característica y el target en un problema de regresión. \n",
    "\n",
    "Calcula la relación entre cada característica independiente y la variable dependiente.\n",
    "\n",
    "La prueba F compara el modelo de regresión que incluye una característica con un modelo que no la incluye.\n",
    "\n",
    "Devuelve dos valores: la estadística F y el valor p correspondiente para cada característica. Un valor más alto de la estadística F indica una relación más fuerte entre la característica y la variable objetivo. Un valor p bajo indica que es poco probable que la característica no tenga una relación con la variable objetivo, es decir, la característica es relevante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamos con respecto al R^2 para observar cuál es el número de variables a seleccionar que nos presentan un mejor resultado.\n",
    "\n",
    "Iteramos de 1 al número máximo de variables (en este caso 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "results = [r2_metrics(make_pipeline(SelectKBest(score_func=f_regression, k=i),\n",
    "                                                 StandardScaler(),\n",
    "                                                 KNeighborsRegressor(n_neighbors=9)),\n",
    "            X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])[1] for i in range(1,13)]\n",
    "ax = sns.lineplot(x=range(1,13),\n",
    "             y=results)\n",
    "ax.set(title=\"R^2 vs K selected features\",ylabel='R^2',xlabel='K selected features')\n",
    "kvals = [(col,i) for col,i in zip(results,range(1,13))]\n",
    "\n",
    "print(sorted(kvals, key=lambda x: x[0],reverse=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el mejor modelo y sus métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "# Guardamos el mejor modelo utilizando k=9 con feature selection (con 5 elegidas), \n",
    "# imputación con la media y eliminación de variables altamente correlacionadas\n",
    "\n",
    "model,metrics = regression_metrics(make_pipeline(SelectKBest(score_func=f_regression, k=5),\n",
    "                                                 StandardScaler(),\n",
    "                                                 KNeighborsRegressor(n_neighbors=9)),\n",
    "                                   X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])\n",
    "pickle.dump(model, open('models/KnnRegressorFSImpMeanRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/KnnRegressorFSImpMeanRemMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora trabajamos con el segundo dataframe, en el que los datos se imputaron con knn con k=19 y se eliminaron las variables altamente correlacionadas entre sí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos encontrar el mejor k para nuestro modelo\n",
    "# Utilizamos un pipeline para incluir la estandarización y el regresor knn \n",
    "\n",
    "for k in range(1,21,2):\n",
    "    print('Results for k =',k)\n",
    "    knnReg_st = make_pipeline(StandardScaler(),KNeighborsRegressor(n_neighbors=k))\n",
    "    regression_metrics(knnReg_st,X_train['knn_rem'],X_test['knn_rem'],y_train['knn_rem'],y_test['knn_rem'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos el R^2\n",
    "\n",
    "results = [r2_metrics(make_pipeline(StandardScaler(),KNeighborsRegressor(n_neighbors=i)),\n",
    "            X_train['knn_rem'],X_test['knn_rem'],y_train['knn_rem'],y_test['knn_rem'])[1] for i in range(1,21,2)]\n",
    "ax = sns.lineplot(x=range(1,21,2),\n",
    "             y=results)\n",
    "ax.set(title=\"R^2 vs K neighbors\",ylabel='R^2',xlabel='K neighbors')\n",
    "kvals = [(col,i) for col,i in zip(results,range(1,21,2))]\n",
    "\n",
    "print(sorted(kvals, key=lambda x: x[0],reverse=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,metrics = regression_metrics(make_pipeline(StandardScaler(),KNeighborsRegressor(n_neighbors=7)),\n",
    "                                   X_train['knn_rem'],X_test['knn_rem'],y_train['knn_rem'],y_test['knn_rem'])\n",
    "pickle.dump(model, open('models/KnnRegressorImpKnnRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/KnnRegressorImpKnnRemMetrics.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestRegressor(n_estimators=100)\n",
    "selector = RFECV(estimator, step=1, cv=5)\n",
    "selector = selector.fit(X_train['knn_rem'], y_train['knn_rem'])\n",
    " \n",
    "print(\"Ranking de características\", selector.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [r2_metrics(make_pipeline(SelectKBest(score_func=f_regression, k=i),\n",
    "                                                 StandardScaler(),\n",
    "                                                 KNeighborsRegressor(n_neighbors=7)),\n",
    "            X_train['knn_rem'],X_test['knn_rem'],y_train['knn_rem'],y_test['knn_rem'])[1] for i in range(1,13)]\n",
    "ax = sns.lineplot(x=range(1,13),\n",
    "             y=results)\n",
    "ax.set(title=\"R^2 vs K selected features\",ylabel='R^2',xlabel='K selected features')\n",
    "kvals = [(col,i) for col,i in zip(results,range(1,13))]\n",
    "\n",
    "print(sorted(kvals, key=lambda x: x[0],reverse=True)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de K vecinos más cercanos con k=5, feature selection y datos imputados con knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el mejor modelo utilizando k=7 con feature selection (con 5 elegidas), \n",
    "# imputación con knn y eliminación de variables altamente correlacionadas\n",
    "\n",
    "model,metrics = regression_metrics(make_pipeline(SelectKBest(score_func=f_regression, k=5),\n",
    "                                                 StandardScaler(),\n",
    "                                                 KNeighborsRegressor(n_neighbors=7)),\n",
    "                                   X_train['knn_rem'],X_test['knn_rem'],y_train['knn_rem'],y_test['knn_rem'])\n",
    "pickle.dump(model, open('models/KnnRegressorFSImpKnnRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/KnnRegressorFSImpKnnRemMetrics.pkl','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Modelo de árbol de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos un árbol de regresión\n",
    "# Verificamos sus métricas\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "regression_metrics(DecisionTreeRegressor(random_state=1234),X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=1234).fit(X_train['mean_rem'],y_train['mean_rem'])\n",
    "\n",
    "# Diagramamos los primeros tres niveles del árbol resultante\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plot_tree(tree_reg,filled=False,fontsize=14,max_depth=3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búsqueda del mejor modelo para árbol de decisión con GridSearchCV\n",
    "- Conjunto de datos imputados con la media y variables altamente correlacionadas entre sí eliminadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a realizar un ajuste de hiperparámetros para encontrar el mejor modelo de árbol de regresión\n",
    "# Establecemos la grilla de parámetros que verificar\n",
    "# Haciendo uso de estos parámetros y una validación cruzada de 3 doblajes buscamos el mejor modelo\n",
    "# Tiempo de ejecución aprox: 12s\n",
    "\n",
    "parameters={\"splitter\":[\"best\",\"random\"],\n",
    "            \"max_depth\" : [1,3,5,7,9,11],\n",
    "           \"min_samples_leaf\":[1,2,3,4,],\n",
    "           \"max_leaf_nodes\":[None,10,20,30] }\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tuning_model=GridSearchCV(DecisionTreeRegressor(random_state=1234),param_grid=parameters,scoring='r2',cv=3,verbose=3)\n",
    "\n",
    "tuning_model.fit(X_train['mean_rem'],y_train['mean_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los híperparámetros del mejor modelo\n",
    "\n",
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El R^2 del mejor modelo\n",
    "\n",
    "tuning_model.best_score_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de árbol de regresión con datos imputados con la media y variables altamente correlacionadas entre sí eliminadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El mejor modelo de árbol de regresión\n",
    "# Con todas datos imputados con la media y eliminación de variables altamente correlacionadas\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=1234,max_depth=9,\n",
    "                                         max_leaf_nodes=None,\n",
    "                                         min_samples_leaf=4,\n",
    "                                         splitter='best')\n",
    "\n",
    "model,metrics = regression_metrics(tree_reg,X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])\n",
    "pickle.dump(model, open('models/DecissionTreeRegressorImpMeanrem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/DecisionTreeRegressorImpMeanRemMetrics.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La visualización del mejor árbol\n",
    "\n",
    "tree_reg.fit(X_train['mean_rem'],y_train['mean_rem'])\n",
    "plt.figure(figsize=(40,40))\n",
    "plot_tree(tree_reg,filled=False,fontsize=14,max_depth=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búsqueda del mejor modelo para árbol de decisión con GridSearchCV\n",
    "- Conjunto de datos con feature selection, datos imputados con la media y eliminación de variables altamente correlacionadas entre sí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo de ejecución aprox: 10s\n",
    "\n",
    "parameters={\"splitter\":[\"best\",\"random\"],\n",
    "            \"max_depth\" : [1,3,5,7,9,11],\n",
    "           \"min_samples_leaf\":[1,2,3,4,],\n",
    "           \"max_leaf_nodes\":[None,10,20,30] }\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_t = SelectKBest(score_func=f_regression, k=5).fit_transform(X_train['mean_rem'],y_train['mean_rem'])                                 \n",
    "\n",
    "tuning_model=GridSearchCV(DecisionTreeRegressor(random_state=1234),param_grid=parameters,scoring='r2',cv=3,verbose=3)\n",
    "tuning_model.fit(X_t,y_train['mean_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_score_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de árbol de regresión con feature selection, datos imputados con la media y eliminación de variables altamente correlacionadas entre sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe_dt = make_pipeline(SelectKBest(score_func=f_regression, k=5),\n",
    "              DecisionTreeRegressor(random_state=1234,max_depth=9,\n",
    "                                         max_leaf_nodes=None,\n",
    "                                         min_samples_leaf=4,\n",
    "                                         splitter='best'))\n",
    "\n",
    "model,metrics = regression_metrics(pipe_dt,X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])\n",
    "pickle.dump(model, open('models/DecissionTreeRegressorFSImpMeanRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/DecisionTreeRegressorFSImpMeanRemMetrics.pkl','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búsqueda del mejor modelo para árbol de decisión con GridSearchCV\n",
    "- Conjunto de datos con datos imputados con knn y eliminación de variables altamente correlacionadas entre sí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo de ejecución aprox: 9s\n",
    "\n",
    "parameters={\"splitter\":[\"best\",\"random\"],\n",
    "            \"max_depth\" : [1,3,5,7,9,11],\n",
    "           \"min_samples_leaf\":[1,2,3,4,],\n",
    "           \"max_leaf_nodes\":[None,10,20,30] }\n",
    "\n",
    "tuning_model=GridSearchCV(DecisionTreeRegressor(random_state=1234),param_grid=parameters,scoring='r2',cv=3,verbose=3)\n",
    "\n",
    "tuning_model.fit(X_train['knn_rem'],y_train['knn_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_score_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de árbol de regresión con datos imputados con knn y eliminación de variables altamente correlacionadas entre sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor(random_state=1234,max_depth=11,\n",
    "                                         max_leaf_nodes=None,\n",
    "                                         min_samples_leaf=3,\n",
    "                                         splitter='best')\n",
    "\n",
    "model,metrics = regression_metrics(tree_reg,X_train['knn_rem'],X_test['knn_rem'],y_train['knn_rem'],y_test['knn_rem'])\n",
    "pickle.dump(model, open('models/DecissionTreeRegressorImpKnnRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/DecisionTreeRegressorImpKnnRemMetrics.pkl','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búsqueda del mejor modelo para árbol de decisión con GridSearchCV\n",
    "- Conjunto de datos con feature selection y datos imputados con knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo de ejecución aprox: 10s\n",
    "\n",
    "parameters={\"splitter\":[\"best\",\"random\"],\n",
    "            \"max_depth\" : [1,3,5,7,9,11],\n",
    "           \"min_samples_leaf\":[1,2,3,4,],\n",
    "           \"max_leaf_nodes\":[None,10,20,30] }\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_t = SelectKBest(score_func=f_regression, k=5).fit_transform(X_train['knn_rem'],y_train['knn_rem'])                                 \n",
    "\n",
    "tuning_model=GridSearchCV(DecisionTreeRegressor(random_state=1234),param_grid=parameters,scoring='r2',cv=3,verbose=3)\n",
    "tuning_model.fit(X_t,y_train['knn_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_score_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de árbol de regresión con feature selection y datos imputados con knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_dt = make_pipeline(SelectKBest(score_func=f_regression, k=5),\n",
    "              DecisionTreeRegressor(random_state=1234,max_depth=9,\n",
    "                                         max_leaf_nodes=None,\n",
    "                                         min_samples_leaf=4,\n",
    "                                         splitter='best'))\n",
    "\n",
    "model,metrics = regression_metrics(pipe_dt,X_train['knn_rem'],X_test['knn_rem'],y_train['knn_rem'],y_test['knn_rem'])\n",
    "pickle.dump(model, open('models/DecissionTreeRegressorFSImpKnnRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/DecisionTreeRegressorFSImpKnnRemMetrics.pkl','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Modelo de Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos trabajando con los datos imputados con la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(random_state=1234)\n",
    "\n",
    "regression_metrics(rf,X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])\n",
    "\n",
    "rf.fit(X_train['mean_rem'],y_train['mean_rem'])\n",
    "print(rf.get_params())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búsqueda del mejor modelo para Random Forest con GridSearchCV\n",
    "- Conjunto con datos imputados con la media y eliminación de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo de ejecución aprox: 4m\n",
    "\n",
    "params = {'max_depth': [10, 20],\n",
    "          'min_samples_leaf': [2, 4],\n",
    "          'min_samples_split': [2, 5],\n",
    "          'n_estimators': [200, 400]}\n",
    "\n",
    "tuning_model=GridSearchCV(RandomForestRegressor(random_state=1234),param_grid=params,scoring='r2',cv=3,verbose=3)\n",
    "\n",
    "tuning_model.fit(X_train['mean_rem'],y_train['mean_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_score_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de Random Forest con datos imputados con la media y eliminación de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El mejor modelo de Random Forest\n",
    "# Con imputación con la media y elimnación de variables\n",
    "\n",
    "rf_reg = RandomForestRegressor(random_state=1234,max_depth=20,\n",
    "                                         min_samples_leaf=2,\n",
    "                                         min_samples_split=2,\n",
    "                                         n_estimators=400)\n",
    "\n",
    "model,metrics = regression_metrics(rf_reg,X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])\n",
    "pickle.dump(model, open('models/RandomForestRegressorImpMeanRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/RandomForestRegressorImpMeanRemMetrics.pkl','wb'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búsqueda del mejor modelo para Random Forest con GridSearchCV\n",
    "- Conjunto de datos con feature selection y datos imputados con la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo de ejecución aprox: 2m\n",
    "\n",
    "params = {'max_depth': [10, 20],\n",
    "          'min_samples_leaf': [2, 4],\n",
    "          'min_samples_split': [2, 5],\n",
    "          'n_estimators': [200, 400]}\n",
    "\n",
    "X_t = SelectKBest(score_func=f_regression,k=5).fit_transform(X_train['mean_rem'],y_train['mean_rem'])\n",
    "\n",
    "tuning_model=GridSearchCV(RandomForestRegressor(random_state=1234),param_grid=params,scoring='r2',cv=3,verbose=3)\n",
    "tuning_model.fit(X_t,y_train['mean_rem'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_score_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de Random Forest con feature selection y datos imputados con la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con feature selection e imputación con la media\n",
    "\n",
    "pipe_rf = make_pipeline(SelectKBest(score_func=f_regression,k=5),\n",
    "                        RandomForestRegressor(random_state=1234,max_depth=20,\n",
    "                                         min_samples_leaf=2,\n",
    "                                         min_samples_split=2,\n",
    "                                         n_estimators=400))\n",
    "\n",
    "model,metrics = regression_metrics(pipe_rf,X_train['knn_rem'],X_test['knn_rem'],y_train['knn_rem'],y_test['knn_rem'])\n",
    "pickle.dump(model, open('models/RandomForestRegressorFSImpMeanRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/RandomForestRegressorFSImpMeanRemMetrics.pkl','wb'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búsqueda del mejor modelo para Random Forest con GridSearchCV\n",
    "- Conjunto con datos imputados con knn y elimnación de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo de ejecución aprox: 4m\n",
    "\n",
    "params = {'max_depth': [10, 20],\n",
    "          'min_samples_leaf': [2, 4],\n",
    "          'min_samples_split': [2, 5],\n",
    "          'n_estimators': [200, 400]}\n",
    "\n",
    "tuning_model=GridSearchCV(RandomForestRegressor(random_state=1234),param_grid=params,scoring='r2',cv=3,verbose=3)\n",
    "\n",
    "tuning_model.fit(X_train['knn_rem'],y_train['knn_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el mejor modelo\n",
    "\n",
    "rf_reg = RandomForestRegressor(random_state=1234,max_depth=20,\n",
    "                                         min_samples_leaf=2,\n",
    "                                         min_samples_split=2,\n",
    "                                         n_estimators=400)\n",
    "\n",
    "model,metrics = regression_metrics(rf_reg,X_train['knn_rem'],X_test['knn_rem'],y_train['knn_rem'],y_test['knn_rem'])\n",
    "pickle.dump(model, open('models/RandomForestRegressorImpKnnRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/RandomForestRegressorImpKnnRemMetrics.pkl','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búsqueda del mejor modelo para Random Forest con GridSearchCV\n",
    "- Conjunto de datos con feature selection y datos imputados con knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo de ejecución aprox: 2m\n",
    "\n",
    "params = {'max_depth': [10, 20],\n",
    "          'min_samples_leaf': [2, 4],\n",
    "          'min_samples_split': [2, 5],\n",
    "          'n_estimators': [200, 400]}\n",
    "\n",
    "tuning_model=GridSearchCV(RandomForestRegressor(random_state=1234),param_grid=params,scoring='r2',cv=3,verbose=3)\n",
    "X_t = SelectKBest(score_func=f_regression,k=5).fit_transform(X_train['knn_rem'],y_train['knn_rem'])\n",
    "tuning_model.fit(X_t,y_train['knn_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_score_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de Random Forest con feature selection y datos imputados con knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con feature selection e imputación con knn\n",
    "\n",
    "\n",
    "pipe_rf = make_pipeline(SelectKBest(score_func=f_regression,k=5),\n",
    "                         RandomForestRegressor(random_state=1234,max_depth=20,\n",
    "                                         min_samples_leaf=2,\n",
    "                                         min_samples_split=2,\n",
    "                                         n_estimators=200))\n",
    "\n",
    "model,metrics = regression_metrics(pipe_rf,X_train['knn_rem'],X_test['knn_rem'],y_train['knn_rem'],y_test['knn_rem'])\n",
    "pickle.dump(model, open('models/RandomForestRegressorFSImpKnnRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/RandomForestRegressorFSImpKnnRemMetrics.pkl','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5. Modelo de Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos imputados con la media\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb = GradientBoostingRegressor(random_state=1234)\n",
    "\n",
    "regression_metrics(gb,X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])\n",
    "\n",
    "gb.fit(X_train['mean_rem'],y_train['mean_rem'])\n",
    "print(gb.get_params())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búsqueda del mejor modelo para Gradient Boosting con GridSearchCV\n",
    "- Conjunto de datos imputados con la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo de ejecución aprox: 6m\n",
    "\n",
    "params = {'learning_rate': [0.01,0.03],\n",
    "                  'subsample'    : [0.5, 0.2],\n",
    "                  'n_estimators' : [500,1000],\n",
    "                  'max_depth'    : [4,6,8]    \n",
    "            }\n",
    "\n",
    "tuning_model=GridSearchCV(GradientBoostingRegressor(random_state=1234),param_grid=params,scoring='r2',cv=3,verbose=3)\n",
    "tuning_model.fit(X_train['mean_rem'],y_train['mean_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_score_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de Gradient Boosting con datos imputados con la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con todas las variables e imputación con la media\n",
    "\n",
    "gb_reg = GradientBoostingRegressor(random_state=1234,learning_rate=0.01,\n",
    "                                         max_depth=8,\n",
    "                                         n_estimators=1000,\n",
    "                                         subsample=0.5)\n",
    "\n",
    "model,metrics = regression_metrics(gb_reg,X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])\n",
    "pickle.dump(model, open('models/GradientBoostingRegressorImpMeanRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/GradientBoostingRegressorImpMeanRemMetrics.pkl','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de Gradient Boosting con feature selection y datos imputados con la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo de ejecución aprox: 3m\n",
    "\n",
    "params = {'learning_rate': [0.01,0.03],\n",
    "                  'subsample'    : [0.5, 0.2],\n",
    "                  'n_estimators' : [500,1000],\n",
    "                  'max_depth'    : [4,6,8]    \n",
    "            }\n",
    "\n",
    "X_t = SelectKBest(score_func=f_regression,k=5).fit_transform(X_train['mean_rem'],y_train['mean_rem'])\n",
    "\n",
    "tuning_model=GridSearchCV(GradientBoostingRegressor(random_state=1234),param_grid=params,scoring='r2',cv=3,verbose=3)\n",
    "tuning_model.fit(X_t,y_train['mean_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con feature selection e imputación con la media\n",
    "\n",
    "pipe_gb = make_pipeline(SelectKBest(score_func=f_regression,k=5),\n",
    "                        GradientBoostingRegressor(random_state=1234,learning_rate=0.01,\n",
    "                                         max_depth=8,\n",
    "                                         n_estimators=500,\n",
    "                                         subsample=0.5))\n",
    "\n",
    "model,metrics = regression_metrics(pipe_gb,X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])\n",
    "pickle.dump(model, open('models/GradientBoostingRegressorFSImpMeanRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/GradientBoostingRegressorFSImpMeanRemMetrics.pkl','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de Gradient Boosting con datos imputados con knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo de ejecución aprox: 9m\n",
    "\n",
    "params = {'learning_rate': [0.01,0.03],\n",
    "                  'subsample'    : [0.5, 0.2],\n",
    "                  'n_estimators' : [500,1000],\n",
    "                  'max_depth'    : [4,6,8]    \n",
    "            }\n",
    "\n",
    "tuning_model=GridSearchCV(GradientBoostingRegressor(random_state=1234),param_grid=params,scoring='r2',cv=3,verbose=3)\n",
    "tuning_model.fit(X_train['knn_rem'],y_train['knn_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con imputación con knn\n",
    "\n",
    "gb_reg = GradientBoostingRegressor(random_state=1234,learning_rate=0.01,\n",
    "                                         max_depth=8,\n",
    "                                         n_estimators=1000,\n",
    "                                         subsample=0.5)\n",
    "\n",
    "model,metrics = regression_metrics(gb_reg,X_train['knn_rem'],X_test['knn_rem'],y_train['knn_rem'],y_test['knn_rem'])\n",
    "pickle.dump(model, open('models/GradientBoostingRegressorImpKnnRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/GradientBoostingRegressorImpKnnRemMetrics.pkl','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de Gradient Boosting con feature selection y datos imputados con knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo de ejecución aprox: 5m\n",
    "\n",
    "params = {'learning_rate': [0.01,0.03],\n",
    "                  'subsample'    : [0.5, 0.2],\n",
    "                  'n_estimators' : [500,1000],\n",
    "                  'max_depth'    : [4,6,8]    \n",
    "            }\n",
    "\n",
    "X_t = SelectKBest(score_func=f_regression,k=5).fit_transform(X_train['knn_rem'],y_train['knn_rem']) \n",
    "\n",
    "tuning_model=GridSearchCV(GradientBoostingRegressor(random_state=1234),param_grid=params,scoring='r2',cv=3,verbose=3)\n",
    "tuning_model.fit(X_t,y_train['knn_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con feature selection e imputación con knn\n",
    "\n",
    "pipe_gb = make_pipeline(SelectKBest(score_func=f_regression,k=5),\n",
    "                        GradientBoostingRegressor(random_state=1234,learning_rate=0.01,\n",
    "                                         max_depth=8,\n",
    "                                         n_estimators=500,\n",
    "                                         subsample=0.5))\n",
    "\n",
    "model,metrics = regression_metrics(pipe_gb,X_train['knn_rem'],X_test['knn_rem'],y_train['knn_rem'],y_test['knn_rem'])\n",
    "pickle.dump(model, open('models/GradientBoostingRegressorFSImpKnnRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/GradientBoostingRegressorFSImpKnnRemMetrics.pkl','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de XGBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBRegressor(random_state=1234)\n",
    "regression_metrics(model,X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búsqueda del mejor modelo para XGBoosting con GridSearchCV\n",
    "- Conjunto de datos con datos imputados con la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo de ejecución aprox: 28m\n",
    "\n",
    "params = {'learning_rate': [0.01,0.03],\n",
    "                  'subsample'    : [0.5, 0.2],\n",
    "                  'n_estimators' : [500,1000],\n",
    "                  'max_depth'    : [4,6,8],\n",
    "                  'gamma'        : (0,1),\n",
    "                  'min_child_weight' : (0,5),\n",
    "                  'scale_pos_weight' : (5,15),\n",
    "                  'max_depth'    : (2,5),\n",
    "                  'alpha'        : (0,10),      \n",
    "                  'eta'          : (0.001,10)    \n",
    "            }\n",
    "\n",
    "tuning_model=GridSearchCV(xgb.XGBRegressor(random_state=1234),\n",
    "                          param_grid=params,scoring='r2',cv=3,verbose=3)\n",
    "tuning_model.fit(X_train['mean_rem'],y_train['mean_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_score_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de XGBoosting con datos imputados con la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con todas las variables e imputación con la media\n",
    "\n",
    "xgb_reg = xgb.XGBRegressor(random_state=1234,learning_rate=0.03,\n",
    "                                         max_depth=5,\n",
    "                                         n_estimators=1000,\n",
    "                                         subsample=0.5,\n",
    "                                         alpha=0,\n",
    "                                         eta=0.001,\n",
    "                                         gamma=0,\n",
    "                                         min_child_weight=0,\n",
    "                                         scale_pos_weight=5)\n",
    "\n",
    "model,metrics = regression_metrics(xgb_reg,X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])\n",
    "pickle.dump(model, open('models/XGBoostingRegressorImpMeanRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/XGBoostingRegressorImpMeanRemMetrics.pkl','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de XGBoosting con feature selection y datos imputados con la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo de ejecución aprox: 30m\n",
    "\n",
    "params = {'learning_rate': [0.01,0.03],\n",
    "                  'subsample'    : [0.5, 0.2],\n",
    "                  'n_estimators' : [500,1000],\n",
    "                  'max_depth'    : [4,6,8],\n",
    "                  'gamma'        : (0,1),\n",
    "                  'min_child_weight' : (0,5),\n",
    "                  'scale_pos_weight' : (5,15),\n",
    "                  'max_depth'    : (2,5),\n",
    "                  'alpha'        : (0,10),      \n",
    "                  'eta'          : (0.001,10)    \n",
    "            }\n",
    "\n",
    "X_t = SelectKBest(score_func=f_regression,k=5).fit_transform(X_train['mean_rem'],y_train['mean_rem'])\n",
    "\n",
    "tuning_model=GridSearchCV(xgb.XGBRegressor(random_state=1234),\n",
    "                          param_grid=params,scoring='r2',cv=3,verbose=3)\n",
    "tuning_model.fit(X_t,y_train['mean_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con feature selection e imputación con la media\n",
    "\n",
    "pipe_xgb = make_pipeline(SelectKBest(score_func=f_regression,k=5),\n",
    "                         xgb.XGBRegressor(random_state=1234,learning_rate=0.03,\n",
    "                                         max_depth=5,\n",
    "                                         n_estimators=500,\n",
    "                                         subsample=0.5,\n",
    "                                         alpha=0,\n",
    "                                         eta=0.001,\n",
    "                                         gamma=0,\n",
    "                                         min_child_weight=0,\n",
    "                                         scale_pos_weight=5))\n",
    "model,metrics = regression_metrics(pipe_xgb,X_train['mean_rem'],X_test['mean_rem'],y_train['mean_rem'],y_test['mean_rem'])\n",
    "pickle.dump(model, open('models/XGBoostingRegressorFSImpMeanRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/XGBoostingRegressorFSImpMeanRemMetrics.pkl','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de XGBoosting con datos imputados con knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo de ejecución aprox: 65m\n",
    "\n",
    "params = {'learning_rate': [0.01,0.03],\n",
    "                  'subsample'    : [0.5, 0.2],\n",
    "                  'n_estimators' : [500,1000],\n",
    "                  'max_depth'    : [4,6,8],\n",
    "                  'gamma'        : (0,1),\n",
    "                  'min_child_weight' : (0,5),\n",
    "                  'scale_pos_weight' : (5,15),\n",
    "                  'max_depth'    : (2,5),\n",
    "                  'alpha'        : (0,10),      \n",
    "                  'eta'          : (0.001,10)    \n",
    "            }\n",
    "\n",
    "tuning_model=GridSearchCV(xgb.XGBRegressor(random_state=1234),\n",
    "                          param_grid=params,scoring='r2',cv=3,verbose=3)\n",
    "tuning_model.fit(X_train['knn_rem'],y_train['knn_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con imputación con knn\n",
    "\n",
    "xgb_reg = xgb.XGBRegressor(random_state=1234,learning_rate=0.03,\n",
    "                                         max_depth=8,\n",
    "                                         n_estimators=1000,\n",
    "                                         subsample=0.5,\n",
    "                                         alpha=0,\n",
    "                                         eta=0.001,\n",
    "                                         gamma=0,\n",
    "                                         min_child_weight=5,\n",
    "                                         scale_pos_weight=5)\n",
    "\n",
    "model,metrics = regression_metrics(xgb_reg,X_train['knn_rem'],X_test['knn_rem'],y_train['knn_rem'],y_test['knn_rem'])\n",
    "pickle.dump(model, open('models/XGBoostingRegressorImpKnnRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/XGBoostingRegressorImpKnnRemMetrics.pkl','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de XGBoosting con feature selection y datos imputados con knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo de ejecución aprox: 34m\n",
    "\n",
    "params = {'learning_rate': [0.01,0.03],\n",
    "                  'subsample'    : [0.5, 0.2],\n",
    "                  'n_estimators' : [500,1000],\n",
    "                  'max_depth'    : [4,6,8],\n",
    "                  'gamma'        : (0,1),\n",
    "                  'min_child_weight' : (0,5),\n",
    "                  'scale_pos_weight' : (5,15),\n",
    "                  'max_depth'    : (2,5),\n",
    "                  'alpha'        : (0,10),      \n",
    "                  'eta'          : (0.001,10)    \n",
    "            }\n",
    "\n",
    "tuning_model=GridSearchCV(xgb.XGBRegressor(random_state=1234),\n",
    "                          param_grid=params,scoring='r2',cv=3,verbose=3)\n",
    "\n",
    "X_t = SelectKBest(score_func=f_regression,k=5).fit_transform(X_train['knn_rem'],y_train['knn_rem'])\n",
    "\n",
    "tuning_model.fit(X_train['knn_rem'],y_train['knn_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con feature selection e imputación con knn\n",
    "\n",
    "pipe_xgb = make_pipeline(SelectKBest(score_func=f_regression,k=5),\n",
    "                         xgb.XGBRegressor(random_state=1234,learning_rate=0.03,\n",
    "                                         max_depth=5,\n",
    "                                         n_estimators=1000,\n",
    "                                         subsample=0.5,\n",
    "                                         alpha=0,\n",
    "                                         eta=0.001,\n",
    "                                         gamma=0,\n",
    "                                         min_child_weight=0,\n",
    "                                         scale_pos_weight=5))\n",
    "model,metrics = regression_metrics(pipe_xgb,X_train['knn_rem'],X_test['knn_rem'],y_train['knn_rem'],y_test['knn_rem'])\n",
    "pickle.dump(model, open('models/XGBoostingRegressorFSImpKnnRem.pkl','wb'))\n",
    "pickle.dump(metrics, open('metrics/XGBoostingRegressorFSImpKnnRemMetrics.pkl','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de los distintos modelos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos las métricas de los modelos previamente guardados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = list()\n",
    "names = list()\n",
    "metrics.append(pickle.load(open('metrics/KnnRegressorImpMeanRemMetrics.pkl','rb')))\n",
    "names.append('KnnRegressorImpMeanRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/KnnRegressorFSImpMeanRemMetrics.pkl','rb')))\n",
    "names.append('KnnRegressorFSImpMeanRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/KnnRegressorImpKnnRemMetrics.pkl','rb')))\n",
    "names.append('KnnRegressorImpKnnRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/KnnRegressorFSImpKnnRemMetrics.pkl','rb')))\n",
    "names.append('KnnRegressorFSImpKnnRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/DecisionTreeRegressorImpMeanRemMetrics.pkl','rb')))\n",
    "names.append('DecisionTreeRegressorImpMeanRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/DecisionTreeRegressorFSImpMeanRemMetrics.pkl','rb')))\n",
    "names.append('DecisionTreeRegressorFSImpMeanRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/DecisionTreeRegressorImpKnnRemMetrics.pkl','rb')))\n",
    "names.append('DecisionTreeRegressorImpKnnRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/DecisionTreeRegressorFSImpKnnRemMetrics.pkl','rb')))\n",
    "names.append('DecisionTreeRegressorFSImpKnnRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/RandomForestRegressorImpMeanRemMetrics.pkl','rb')))\n",
    "names.append('RandomForestRegressorImpMeanRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/RandomForestRegressorFSImpMeanRemMetrics.pkl','rb')))\n",
    "names.append('RandomForestRegressorFSImpMeanRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/RandomForestRegressorImpKnnRemMetrics.pkl','rb')))\n",
    "names.append('RandomForestRegressorImpKnnRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/RandomForestRegressorFSImpKnnRemMetrics.pkl','rb')))\n",
    "names.append('RandomForestRegressorFSImpKnnRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/GradientBoostingRegressorImpMeanRemMetrics.pkl','rb')))\n",
    "names.append('GradientBoostingRegressorImpMeanRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/GradientBoostingRegressorFSImpMeanRemMetrics.pkl','rb')))\n",
    "names.append('GradientBoostingRegressorFSImpMeanRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/GradientBoostingRegressorImpKnnRemMetrics.pkl','rb')))\n",
    "names.append('GradientBoostingRegressorImpKnnRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/GradientBoostingRegressorFSImpKnnRemMetrics.pkl','rb')))\n",
    "names.append('GradientBoostingRegressorFSImpKnnRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/XGBoostingRegressorImpMeanRemMetrics.pkl','rb')))\n",
    "names.append('XGBoostingRegressorImpMeanRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/XGBoostingRegressorFSImpMeanRemMetrics.pkl','rb')))\n",
    "names.append('XGBoostingRegressorFSImpMeanRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/XGBoostingRegressorImpKnnRemMetrics.pkl','rb')))\n",
    "names.append('XGBoostingRegressorImpKnnRemMetrics')\n",
    "metrics.append(pickle.load(open('metrics/XGBoostingRegressorFSImpKnnRemMetrics.pkl','rb')))\n",
    "names.append('XGBoostingRegressorFSImpKnnRemMetrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagrama de barras para observar el modelo más importante de acuerdo al R^2 de prueba\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((20,20))\n",
    "ax.bar(names, [i['Test R^2'] for i in metrics])\n",
    "ax.set(title='R^2 de prueba',xlabel='Modelos',ylabel='Score')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagrama de barras para observar el modelo más importante de acuerdo al R^2 ajustado de prueba\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((20,20))\n",
    "ax.bar(names, [i['Test Adj R^2'] for i in metrics])\n",
    "ax.set(title='R^2 ajustado de prueba',xlabel='Modelos',ylabel='Score')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagrama de barras para observar el modelo más importante de acuerdo al MAE de prueba\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((20,20))\n",
    "ax.bar(names, [i['Test MAE'] for i in metrics])\n",
    "ax.set(title='MAE de prueba',xlabel='Modelos',ylabel='Score')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagrama de barras para observar el modelo más importante de acuerdo al MAPE de prueba\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((20,20))\n",
    "ax.bar(names, [i['Test MAPE'] for i in metrics])\n",
    "ax.set(title='MAPE de prueba',xlabel='Modelos',ylabel='Score')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagrama de barras para observar el modelo más importante de acuerdo al RMSE de prueba\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((20,20))\n",
    "ax.bar(names, [i['Test RMSE'] for i in metrics])\n",
    "ax.set(title='RMSE de prueba',xlabel='Modelos',ylabel='Score')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se organizan los modelos de acuerdo a su R^2 de prueba\n",
    "\n",
    "models = [(names[i],metrics[i]) for i in range(len(names))]\n",
    "models = sorted(models,key=lambda x:x[1]['Test R^2'],reverse=True)\n",
    "ranking = pd.DataFrame(columns=['Model','Test R^2'])\n",
    "for model in models:\n",
    "    ranking = ranking._append({'Model':model[0],'Test R^2':model[1]['Test R^2']},ignore_index=True)\n",
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se organizan los modelos de acuerdo a su R^2 ajustado de prueba\n",
    "\n",
    "models = [(names[i],metrics[i]) for i in range(len(names))]\n",
    "models = sorted(models,key=lambda x:x[1]['Test Adj R^2'],reverse=True)\n",
    "ranking = pd.DataFrame(columns=['Model','Test Adj R^2'])\n",
    "for model in models:\n",
    "    ranking = ranking._append({'Model':model[0],'Test Adj R^2':model[1]['Test Adj R^2']},ignore_index=True)\n",
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se organizan los modelos de acuerdo a su RMSE de prueba\n",
    "\n",
    "models = [(names[i],metrics[i]) for i in range(len(names))]\n",
    "models = sorted(models,key=lambda x:x[1]['Test RMSE'],reverse=False)\n",
    "ranking = pd.DataFrame(columns=['Model','Test RMSE'])\n",
    "for model in models:\n",
    "    ranking = ranking._append({'Model':model[0],'Test RMSE':model[1]['Test RMSE']},ignore_index=True)\n",
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se organizan los modelos de acuerdo a su MAE de prueba\n",
    "\n",
    "models = [(names[i],metrics[i]) for i in range(len(names))]\n",
    "models = sorted(models,key=lambda x:x[1]['Test MAE'],reverse=False)\n",
    "ranking = pd.DataFrame(columns=['Model','Test MAE'])\n",
    "for model in models:\n",
    "    ranking = ranking._append({'Model':model[0],'Test MAE':model[1]['Test MAE']},ignore_index=True)\n",
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se organizan los modelos de acuerdo a su MAPE de prueba\n",
    "\n",
    "models = [(names[i],metrics[i]) for i in range(len(names))]\n",
    "models = sorted(models,key=lambda x:x[1]['Test MAPE'],reverse=False)\n",
    "ranking = pd.DataFrame(columns=['Model','Test MAPE'])\n",
    "for model in models:\n",
    "    ranking = ranking._append({'Model':model[0],'Test MAPE':model[1]['Test MAPE']},ignore_index=True)\n",
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
